{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thiwanka-Sandakalum/ETL-pipline/blob/main/Educational_Data_ETL_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "150d7c8b",
      "metadata": {
        "id": "150d7c8b"
      },
      "source": [
        "# ðŸŽ“ Educational Data ETL Pipeline with LLM Extraction\n",
        "\n",
        "## System Overview\n",
        "\n",
        "```\n",
        "Raw Text Files (per institution)\n",
        "        â†“\n",
        "Chunking + Cleaning\n",
        "        â†“\n",
        "LLM Extraction (Ollama / LLaMA)\n",
        "        â†“\n",
        "Strict JSON Validation (Pydantic)\n",
        "        â†“\n",
        "Confidence Scoring & Normalization\n",
        "        â†“\n",
        "MongoDB Insert (Institutions / Programs / Raw)\n",
        "        â†“\n",
        "Audit + Retry Queue\n",
        "```\n",
        "\n",
        "## Technology Stack\n",
        "\n",
        "| Layer | Technology |\n",
        "|-------|-----------|\n",
        "| LLM Runtime | Ollama (LLaMA 3.x / Mistral) |\n",
        "| Runtime Env | Google Colab / Local |\n",
        "| Language | Python 3.10+ |\n",
        "| Validation | Pydantic |\n",
        "| Database | MongoDB (Atlas or Local) |\n",
        "| Embeddings | Ollama embeddings |\n",
        "| Logging | JSON logs |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59f17522",
      "metadata": {
        "id": "59f17522"
      },
      "source": [
        "## ðŸ“¦ Step 1: Install Dependencies and Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "39113814",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39113814",
        "outputId": "c9302788-e69e-4025-d568-1d647b8c0860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing pymongo[srv]==3.12...\n",
            "Installing pydantic...\n",
            "Installing requests...\n",
            "Installing python-dotenv...\n",
            "âœ… All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Install all required dependencies\"\"\"\n",
        "    packages = [\n",
        "        'pymongo[srv]==3.12',  # MongoDB driver with srv support for Atlas\n",
        "        'pydantic',\n",
        "        'requests',\n",
        "        'python-dotenv'\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "\n",
        "    print(\"âœ… All packages installed successfully!\")\n",
        "\n",
        "install_packages()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6c2df290",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c2df290",
        "outputId": "c30dc7ad-f30d-46c7-97de-bb037fab2b88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŒ Running in Google Colab\n",
            "   ðŸ“‚ Output: /content/output\n",
            "   ðŸ“‚ Input: /content/bci_lk\n",
            "   ðŸ“‚ Base: /content\n"
          ]
        }
      ],
      "source": [
        "# Create necessary directories\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Auto-detect base directory for Google Colab or local environment\n",
        "try:\n",
        "    import google.colab\n",
        "    # Running in Google Colab\n",
        "    BASE_DIR = Path(\"/content\")\n",
        "    print(\"ðŸŒ Running in Google Colab\")\n",
        "except ImportError:\n",
        "    # Running locally - auto-detect from current working directory\n",
        "    BASE_DIR = Path.cwd()\n",
        "    print(\"ðŸ’» Running locally\")\n",
        "\n",
        "INPUT_DIR = BASE_DIR / \"bci_lk\"\n",
        "LOGS_DIR = BASE_DIR / \"logs\"\n",
        "OUTPUT_DIR = BASE_DIR / \"output\"\n",
        "\n",
        "print(f\"   ðŸ“‚ Output: {OUTPUT_DIR}\")\n",
        "\n",
        "# Create directories if they don't existprint(f\"   ðŸ“‚ Logs: {LOGS_DIR}\")\n",
        "\n",
        "LOGS_DIR.mkdir(exist_ok=True)\n",
        "print(f\"   ðŸ“‚ Input: {INPUT_DIR}\")\n",
        "\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "print(f\"   ðŸ“‚ Base: {BASE_DIR}\")\n",
        "\n",
        "INPUT_DIR.mkdir(exist_ok=True)  # Create input dir if neededprint(f\"âœ… Directory structure created:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "40002502",
      "metadata": {
        "id": "40002502"
      },
      "outputs": [],
      "source": [
        "# Import core libraries\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from typing import Optional, List, Dict, Any\n",
        "import traceback\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(LOGS_DIR / f'pipeline_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(\"ðŸš€ ETL Pipeline Initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f9b06b6",
      "metadata": {
        "id": "3f9b06b6"
      },
      "source": [
        "## ðŸ¤– Step 2: Install and Start Ollama Server\n",
        "\n",
        "**Note:** This section is for Google Colab. For local environments, install Ollama separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9b808cbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b808cbe",
        "outputId": "3baf53e9-0202-4e1e-9858-f2d60c163b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“ Running in Google Colab - Installing Ollama...\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tgz\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "âœ… Ollama installed and started successfully!\n"
          ]
        }
      ],
      "source": [
        "# Check if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"ðŸ“ Running in Google Colab - Installing Ollama...\")\n",
        "    # Install Ollama in Colab\n",
        "    !curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "    # Start Ollama server in background\n",
        "    import subprocess\n",
        "    import time\n",
        "\n",
        "    # Start server\n",
        "    ollama_process = subprocess.Popen(['ollama', 'serve'],\n",
        "                                      stdout=subprocess.PIPE,\n",
        "                                      stderr=subprocess.PIPE)\n",
        "    time.sleep(5)  # Wait for server to start\n",
        "\n",
        "    # Pull LLaMA 3 model\n",
        "    !ollama pull llama3\n",
        "\n",
        "    print(\"âœ… Ollama installed and started successfully!\")\n",
        "else:\n",
        "    print(\"ðŸ“ Running locally - Please ensure Ollama is installed and running\")\n",
        "    print(\"   Install: curl -fsSL https://ollama.com/install.sh | sh\")\n",
        "    print(\"   Start server: ollama serve\")\n",
        "    print(\"   Pull model: ollama pull llama3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09ec07d4",
      "metadata": {
        "id": "09ec07d4"
      },
      "outputs": [],
      "source": [
        "# Test Ollama connection\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def test_ollama_connection(max_retries=5):\n",
        "    \"\"\"Test if Ollama server is accessible\"\"\"\n",
        "    for i in range(max_retries):\n",
        "        try:\n",
        "            response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
        "            if response.status_code == 200:\n",
        "                models = response.json().get('models', [])\n",
        "                print(f\"âœ… Ollama server is running!\")\n",
        "                print(f\"   Available models: {[m['name'] for m in models]}\")\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            print(f\"â³ Waiting for Ollama server... (attempt {i+1}/{max_retries})\")\n",
        "            time.sleep(2)\n",
        "\n",
        "    print(\"âŒ Could not connect to Ollama server\")\n",
        "    return False\n",
        "\n",
        "test_ollama_connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718716c1",
      "metadata": {
        "id": "718716c1"
      },
      "source": [
        "## ðŸ“‹ Step 3: Define Pydantic Data Models (JSON Contract)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "db1cce63",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db1cce63",
        "outputId": "275e2c36-21e6-44cd-e33b-05ad94c79a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Pydantic schemas defined successfully!\n",
            "   - Institution\n",
            "   - Program\n",
            "   - ExtractionResult\n",
            "   - ExtractionResult\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel, Field, validator, ConfigDict\n",
        "from typing import Optional, List, Dict, Any\n",
        "from datetime import datetime\n",
        "\n",
        "class Institution(BaseModel):\n",
        "    \"\"\"Strict schema for educational institution data\"\"\"\n",
        "    institution_code: Optional[str] = Field(None, description=\"Unique institution identifier\")\n",
        "    name: str = Field(..., description=\"Official institution name\")\n",
        "    description: Optional[str] = Field(None, description=\"Institution description and overview\")\n",
        "    type: List[str] = Field(default_factory=list, description=\"Institution types (e.g., university, college)\")\n",
        "    country: Optional[str] = Field(default=\"Sri Lanka\", description=\"Country where institution is located\") # Changed to Optional[str]\n",
        "    website: Optional[str] = Field(None, description=\"Official website URL\")\n",
        "    recognition: Optional[Dict[str, Optional[bool]]] = Field(None, description=\"Recognition/accreditation status\")\n",
        "    contact_info: Optional[Dict[str, Any]] = Field(None, description=\"Contact details\")\n",
        "    confidence_score: float = Field(..., ge=0.0, le=1.0, description=\"Extraction confidence (0.0-1.0)\")\n",
        "\n",
        "    model_config = ConfigDict(extra=\"forbid\")  # Use ConfigDict instead of class Config\n",
        "\n",
        "\n",
        "class Program(BaseModel):\n",
        "    \"\"\"Strict schema for academic program data\"\"\"\n",
        "    name: str = Field(..., description=\"Official program name\") # Added this line\n",
        "    program_code: Optional[str] = Field(None, description=\"Unique program identifier\")\n",
        "    description: Optional[str] = Field(None, description=\"Program description and overview\")\n",
        "    level: Optional[str] = Field(None, description=\"Academic level (e.g., Bachelor, Diploma)\")\n",
        "    duration: Optional[Dict[str, Any]] = Field(None, description=\"Program duration details\")\n",
        "    delivery_mode: Optional[List[str]] = Field(None, description=\"Delivery modes (online, on-campus, hybrid)\")\n",
        "    fees: Optional[Dict[str, Any]] = Field(None, description=\"Fee structure\")\n",
        "    eligibility: Optional[Dict[str, Any]] = Field(None, description=\"Admission requirements\")\n",
        "    curriculum_summary: Optional[str] = Field(None, description=\"Brief curriculum overview\")\n",
        "    specializations: Optional[List[str]] = Field(None, description=\"Available specializations\")\n",
        "    extensions: Optional[Dict[str, Any]] = Field(None, description=\"Additional unstructured data\")\n",
        "    confidence_score: float = Field(..., ge=0.0, le=1.0, description=\"Extraction confidence (0.0-1.0)\")\n",
        "\n",
        "    model_config = ConfigDict(extra=\"forbid\") # Use ConfigDict instead of class Config\n",
        "\n",
        "\n",
        "class ExtractionResult(BaseModel):\n",
        "    \"\"\"Complete extraction result container\"\"\"\n",
        "    institution: Institution\n",
        "    programs: List[Program] = Field(default_factory=list)\n",
        "    raw_text: str = Field(..., description=\"Original source text\")\n",
        "    extraction_timestamp: datetime = Field(default_factory=datetime.now)\n",
        "    source_file: Optional[str] = Field(None, description=\"Source filename\")\n",
        "\n",
        "    model_config = ConfigDict(extra=\"allow\")  # Use ConfigDict instead of class Config\n",
        "\n",
        "print(\"âœ… Pydantic schemas defined successfully!\")\n",
        "print(f\"   - Institution\")\n",
        "print(f\"   - Program\")\n",
        "\n",
        "print(f\"   - ExtractionResult\")\n",
        "print(f\"   - ExtractionResult\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa731ba",
      "metadata": {
        "id": "5aa731ba"
      },
      "source": [
        "## ðŸ’¬ Step 4: Create Prompt Engineering Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cffb965e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cffb965e",
        "outputId": "a5f347bf-e672-414a-ee0d-42e6c499fc96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Extraction prompts configured\n",
            "   System prompt length: 1316 chars\n",
            "   User template ready\n"
          ]
        }
      ],
      "source": [
        "SYSTEM_PROMPT = \"\"\"You are a structured data extraction engine for educational institutions.\n",
        "\n",
        "CRITICAL RULES:\n",
        "1. Output ONLY valid JSON - no markdown, no code blocks, no explanations\n",
        "2. Follow the JSON schema EXACTLY as defined\n",
        "3. Do NOT infer, guess, or hallucinate information\n",
        "4. If data is missing or unclear, OMIT the field entirely\n",
        "5. Unknown or unstructured information â†’ place in \"extensions\" field\n",
        "6. Preserve original wording and terminology from source\n",
        "7. Assign confidence_score honestly based on data clarity (0.0â€“1.0)\n",
        "8. Be conservative with confidence scores - unclear data = lower score\n",
        "\n",
        "CONFIDENCE SCORING GUIDELINES:\n",
        "- 1.0: Explicitly stated, clear, unambiguous\n",
        "- 0.8-0.9: Clearly stated but minor ambiguity\n",
        "- 0.6-0.7: Implied or partially stated\n",
        "- 0.4-0.5: Inferred from context\n",
        "- 0.0-0.3: Highly uncertain or speculative\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "Return a single JSON object with this structure:\n",
        "{\n",
        "  \"institution\": {\n",
        "    \"name\": \"...\",\n",
        "    \"description\": \"Institution overview and mission...\",\n",
        "    \"type\": [\"...\"],\n",
        "    \"country\": \"...\",\n",
        "    \"website\": \"...\",\n",
        "    \"confidence_score\": 0.0-1.0\n",
        "  },\n",
        "  \"programs\": [\n",
        "    {\n",
        "      \"name\": \"...\",\n",
        "      \"description\": \"Program overview and objectives...\",\n",
        "      \"level\": \"...\",\n",
        "      \"duration\": {...},\n",
        "      \"curriculum_summary\": \"...\",\n",
        "      \"confidence_score\": 0.0-1.0\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "USER_PROMPT_TEMPLATE = \"\"\"Extract structured educational data from the following content.\n",
        "\n",
        "Source content:\n",
        "<<<\n",
        "{content}\n",
        ">>>\n",
        "\n",
        "Return ONLY the JSON object. No explanations. No markdown.\n",
        "\"\"\"\n",
        "\n",
        "print(\"âœ… Extraction prompts configured\")\n",
        "print(f\"   System prompt length: {len(SYSTEM_PROMPT)} chars\")\n",
        "print(f\"   User template ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "954b59a0",
      "metadata": {
        "id": "954b59a0"
      },
      "source": [
        "## ðŸ”Œ Step 5: Implement Ollama LLM Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c5e315d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5e315d9",
        "outputId": "70148648-b2e5-4c49-8ecf-3b32683096f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ollama client initialized and connected\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "\n",
        "class OllamaClient:\n",
        "    \"\"\"Robust Ollama LLM client with error handling\"\"\"\n",
        "\n",
        "    def __init__(self, base_url=\"http://localhost:11434\", model=\"llama3\"):\n",
        "        self.base_url = base_url\n",
        "        self.model = model\n",
        "        self.api_url = f\"{base_url}/api/generate\"\n",
        "\n",
        "    def call(self, prompt: str, temperature: float = 0.1, timeout: int = 300) -> dict:\n",
        "        \"\"\"\n",
        "        Call Ollama API with structured prompt\n",
        "\n",
        "        Args:\n",
        "            prompt: Complete prompt including system and user messages\n",
        "            temperature: Sampling temperature (lower = more deterministic)\n",
        "            timeout: Request timeout in seconds\n",
        "\n",
        "        Returns:\n",
        "            Parsed JSON response from LLM\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If JSON parsing fails\n",
        "            requests.RequestException: If API call fails\n",
        "        \"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Calling Ollama with model: {self.model}\")\n",
        "\n",
        "            response = requests.post(\n",
        "                self.api_url,\n",
        "                json={\n",
        "                    \"model\": self.model,\n",
        "                    \"prompt\": prompt,\n",
        "                    \"stream\": False,\n",
        "                    \"temperature\": temperature,\n",
        "                    \"options\": {\n",
        "                        \"num_predict\": 4096,  # Max tokens\n",
        "                        \"top_k\": 40,\n",
        "                        \"top_p\": 0.9\n",
        "                    }\n",
        "                },\n",
        "                timeout=timeout\n",
        "            )\n",
        "\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Extract response text\n",
        "            raw_response = response.json().get(\"response\", \"\")\n",
        "            logger.debug(f\"Raw LLM response: {raw_response[:200]}...\")\n",
        "\n",
        "            # Clean response (remove markdown code blocks if present)\n",
        "            cleaned = self._clean_json_response(raw_response)\n",
        "\n",
        "            # Parse JSON\n",
        "            try:\n",
        "                return json.loads(cleaned)\n",
        "            except json.JSONDecodeError as e:\n",
        "                logger.error(f\"JSON decode error: {e}\")\n",
        "                logger.error(f\"Cleaned response: {cleaned[:500]}\")\n",
        "                raise ValueError(f\"Invalid JSON returned by LLM: {e}\")\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            logger.error(f\"Ollama API request failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _clean_json_response(self, text: str) -> str:\n",
        "        \"\"\"Remove markdown code blocks and extract JSON\"\"\"\n",
        "        # Remove markdown code blocks\n",
        "        text = re.sub(r'```json\\s*', '', text)\n",
        "        text = re.sub(r'```\\s*', '', text)\n",
        "\n",
        "        # Find JSON object (starting with { and ending with })\n",
        "        match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(0)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def test_connection(self) -> bool:\n",
        "        \"\"\"Test if Ollama server is accessible\"\"\"\n",
        "        try:\n",
        "            response = requests.get(f\"{self.base_url}/api/tags\", timeout=5)\n",
        "            return response.status_code == 200\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "# Initialize client\n",
        "ollama_client = OllamaClient(model=\"llama3\")\n",
        "\n",
        "# Test connection\n",
        "if ollama_client.test_connection():\n",
        "    print(\"âœ… Ollama client initialized and connected\")\n",
        "else:\n",
        "    print(\"âš ï¸  Warning: Could not connect to Ollama server\")\n",
        "    print(\"   Make sure Ollama is running: ollama serve\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8cd34c5",
      "metadata": {
        "id": "f8cd34c5"
      },
      "source": [
        "## âœ‚ï¸ Step 6: Build Text Chunking Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "57a52803",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57a52803",
        "outputId": "97ef896a-f8d2-4480-995a-f57873fc1f32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Text chunker initialized\n",
            "   Test: 8000 chars â†’ 3 chunks\n",
            "   Max chunk size: 2999 chars\n"
          ]
        }
      ],
      "source": [
        "class TextChunker:\n",
        "    \"\"\"Intelligent text chunking for large documents\"\"\"\n",
        "\n",
        "    def __init__(self, max_chars=3000, overlap=200):\n",
        "        \"\"\"\n",
        "        Initialize chunker\n",
        "\n",
        "        Args:\n",
        "            max_chars: Maximum characters per chunk\n",
        "            overlap: Character overlap between chunks for context preservation\n",
        "        \"\"\"\n",
        "        self.max_chars = max_chars\n",
        "        self.overlap = overlap\n",
        "\n",
        "    def chunk_text(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into manageable chunks while preserving context\n",
        "\n",
        "        Args:\n",
        "            text: Input text to chunk\n",
        "\n",
        "        Returns:\n",
        "            List of text chunks\n",
        "        \"\"\"\n",
        "        if len(text) <= self.max_chars:\n",
        "            return [text]\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = \"\"\n",
        "\n",
        "        # Split by paragraphs (double newline) for better context\n",
        "        paragraphs = text.split('\\n\\n')\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "            # If single paragraph exceeds max, split by sentences\n",
        "            if len(paragraph) > self.max_chars:\n",
        "                sentences = paragraph.split('. ')\n",
        "                for sentence in sentences:\n",
        "                    if len(current_chunk) + len(sentence) + 2 <= self.max_chars:\n",
        "                        current_chunk += sentence + '. '\n",
        "                    else:\n",
        "                        if current_chunk:\n",
        "                            chunks.append(current_chunk.strip())\n",
        "                            # Add overlap from previous chunk\n",
        "                            current_chunk = current_chunk[-self.overlap:] + sentence + '. '\n",
        "                        else:\n",
        "                            current_chunk = sentence + '. '\n",
        "            else:\n",
        "                # Add full paragraph if it fits\n",
        "                if len(current_chunk) + len(paragraph) + 2 <= self.max_chars:\n",
        "                    current_chunk += paragraph + '\\n\\n'\n",
        "                else:\n",
        "                    if current_chunk:\n",
        "                        chunks.append(current_chunk.strip())\n",
        "                        # Add overlap\n",
        "                        current_chunk = current_chunk[-self.overlap:] + paragraph + '\\n\\n'\n",
        "                    else:\n",
        "                        current_chunk = paragraph + '\\n\\n'\n",
        "\n",
        "        # Add final chunk\n",
        "        if current_chunk:\n",
        "            chunks.append(current_chunk.strip())\n",
        "\n",
        "        logger.info(f\"Split text into {len(chunks)} chunks\")\n",
        "        return chunks\n",
        "\n",
        "    def clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text\"\"\"\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # Remove special characters but keep punctuation\n",
        "        text = re.sub(r'[^\\w\\s\\.\\,\\:\\;\\-\\(\\)\\[\\]\\/\\&]', '', text)\n",
        "        return text.strip()\n",
        "\n",
        "# Initialize chunker\n",
        "chunker = TextChunker(max_chars=3000, overlap=200)\n",
        "\n",
        "# Test chunking\n",
        "test_text = \"This is a test. \" * 500\n",
        "test_chunks = chunker.chunk_text(test_text)\n",
        "print(f\"âœ… Text chunker initialized\")\n",
        "print(f\"   Test: {len(test_text)} chars â†’ {len(test_chunks)} chunks\")\n",
        "print(f\"   Max chunk size: {max(len(c) for c in test_chunks)} chars\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1332fb3f",
      "metadata": {
        "id": "1332fb3f"
      },
      "source": [
        "## âœ… Step 7: Implement JSON Validation Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4b88b055",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b88b055",
        "outputId": "bdd23ef4-b395-4568-fe9f-87b079e8a670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Validation layer ready\n"
          ]
        }
      ],
      "source": [
        "from pydantic import ValidationError\n",
        "\n",
        "class DataValidator:\n",
        "    \"\"\"Strict Pydantic-based validation\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_extraction(data: dict, source_file: str = None) -> ExtractionResult:\n",
        "        \"\"\"\n",
        "        Validate extracted data against Pydantic schema\n",
        "\n",
        "        Args:\n",
        "            data: Raw dictionary from LLM\n",
        "            source_file: Source filename for metadata\n",
        "\n",
        "        Returns:\n",
        "            Validated ExtractionResult object\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If validation fails\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Add metadata\n",
        "            if source_file:\n",
        "                data['source_file'] = source_file\n",
        "\n",
        "            # Validate against schema\n",
        "            result = ExtractionResult(**data)\n",
        "\n",
        "            logger.info(f\"âœ… Validation passed\")\n",
        "            logger.info(f\"   Institution: {result.institution.name}\")\n",
        "            logger.info(f\"   Programs: {len(result.programs)}\")\n",
        "\n",
        "            return result\n",
        "\n",
        "        except ValidationError as e:\n",
        "            error_details = []\n",
        "            for error in e.errors():\n",
        "                field = '.'.join(str(x) for x in error['loc'])\n",
        "                message = error['msg']\n",
        "                error_details.append(f\"  - {field}: {message}\")\n",
        "\n",
        "            error_msg = \"Schema validation failed:\\n\" + \"\\n\".join(error_details)\n",
        "            logger.error(error_msg)\n",
        "            raise ValueError(error_msg)\n",
        "\n",
        "    @staticmethod\n",
        "    def validate_confidence_scores(result: ExtractionResult) -> bool:\n",
        "        \"\"\"Check if confidence scores are within acceptable range\"\"\"\n",
        "        min_acceptable = 0.5\n",
        "\n",
        "        # Check institution confidence\n",
        "        if result.institution.confidence_score < min_acceptable:\n",
        "            logger.warning(f\"Low institution confidence: {result.institution.confidence_score}\")\n",
        "            return False\n",
        "\n",
        "        # Check program confidences\n",
        "        low_confidence_programs = [\n",
        "            p.name for p in result.programs\n",
        "            if p.confidence_score < min_acceptable\n",
        "        ]\n",
        "\n",
        "        if low_confidence_programs:\n",
        "            logger.warning(f\"Low confidence programs: {low_confidence_programs}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "validator = DataValidator()\n",
        "print(\"âœ… Validation layer ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bac39b4",
      "metadata": {
        "id": "3bac39b4"
      },
      "source": [
        "## ðŸŽ¯ Step 8: Create Confidence Scoring System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ae6905e6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae6905e6",
        "outputId": "3c0291ea-712b-40ec-aeb1-5c370abcfd10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Confidence scoring system ready\n"
          ]
        }
      ],
      "source": [
        "class ConfidenceScorer:\n",
        "    \"\"\"Normalize and adjust confidence scores based on data quality\"\"\"\n",
        "\n",
        "    def __init__(self, low_confidence_threshold=0.6, penalty_factor=0.9):\n",
        "        self.low_confidence_threshold = low_confidence_threshold\n",
        "        self.penalty_factor = penalty_factor\n",
        "\n",
        "    def normalize_confidence(self, result: ExtractionResult) -> ExtractionResult:\n",
        "        \"\"\"\n",
        "        Apply confidence normalization rules\n",
        "\n",
        "        Rules:\n",
        "        1. Penalize scores below threshold\n",
        "        2. Adjust based on data completeness\n",
        "        3. Apply penalties for missing critical fields\n",
        "\n",
        "        Args:\n",
        "            result: ExtractionResult to normalize\n",
        "\n",
        "        Returns:\n",
        "            Normalized ExtractionResult\n",
        "        \"\"\"\n",
        "        # Normalize institution confidence\n",
        "        result.institution.confidence_score = self._normalize_institution_score(\n",
        "            result.institution\n",
        "        )\n",
        "\n",
        "        # Normalize program confidences\n",
        "        for program in result.programs:\n",
        "            program.confidence_score = self._normalize_program_score(program)\n",
        "\n",
        "        logger.info(f\"Confidence scores normalized\")\n",
        "        logger.info(f\"   Institution: {result.institution.confidence_score:.2f}\")\n",
        "        logger.info(f\"   Programs avg: {sum(p.confidence_score for p in result.programs) / len(result.programs):.2f}\"\n",
        "                   if result.programs else \"   Programs: N/A\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _normalize_institution_score(self, institution: Institution) -> float:\n",
        "        \"\"\"Normalize institution confidence score\"\"\"\n",
        "        score = institution.confidence_score\n",
        "\n",
        "        # Apply low confidence penalty\n",
        "        if score < self.low_confidence_threshold:\n",
        "            score *= self.penalty_factor\n",
        "\n",
        "        # Penalize if critical fields are missing\n",
        "        if not institution.website:\n",
        "            score *= 0.95\n",
        "\n",
        "        if not institution.institution_code:\n",
        "            score *= 0.98\n",
        "\n",
        "        return round(min(score, 1.0), 3)\n",
        "\n",
        "    def _normalize_program_score(self, program: Program) -> float:\n",
        "        \"\"\"Normalize program confidence score\"\"\"\n",
        "        score = program.confidence_score\n",
        "\n",
        "        # Apply low confidence penalty\n",
        "        if score < self.low_confidence_threshold:\n",
        "            score *= 0.85  # Stricter for programs\n",
        "\n",
        "        # Penalize missing important fields\n",
        "        missing_fields = 0\n",
        "        if not program.level:\n",
        "            missing_fields += 1\n",
        "        if not program.duration:\n",
        "            missing_fields += 1\n",
        "        if not program.curriculum_summary:\n",
        "            missing_fields += 1\n",
        "\n",
        "        # Reduce score by 3% per missing important field\n",
        "        score *= (1.0 - 0.03 * missing_fields)\n",
        "\n",
        "        return round(min(score, 1.0), 3)\n",
        "\n",
        "        # Apply penalty if missing qualifications\n",
        "    def get_overall_quality_score(self, result: ExtractionResult) -> float:\n",
        "        \"\"\"Calculate overall data quality score\"\"\"\n",
        "        scores = [result.institution.confidence_score]\n",
        "        scores.extend([p.confidence_score for p in result.programs])\n",
        "\n",
        "        return sum(scores) / len(scores) if scores else 0.0\n",
        "\n",
        "scorer = ConfidenceScorer()\n",
        "print(\"âœ… Confidence scoring system ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1867c08c",
      "metadata": {
        "id": "1867c08c"
      },
      "source": [
        "## ðŸ’¾ Step 9: Setup MongoDB Connection and Writer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1042bfbd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1042bfbd",
        "outputId": "fd64b239-2d4a-489c-fbfe-8aca97472fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… MongoDB Atlas configured\n",
            "   Database: edu_platform\n",
            "   Cluster: development.ps1jayw.mongodb.net\n"
          ]
        }
      ],
      "source": [
        "# MongoDB Atlas Configuration\n",
        "MONGODB_URI = \"mongodb+srv://ict22006_db_user:gGgnHUqamNcU5jAy@development.ps1jayw.mongodb.net/?appName=development\"\n",
        "DATABASE_NAME = \"edu_platform\"\n",
        "\n",
        "print(\"âœ… MongoDB Atlas configured\")\n",
        "print(f\"   Database: {DATABASE_NAME}\")\n",
        "print(f\"   Cluster: development.ps1jayw.mongodb.net\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "39c69544",
      "metadata": {
        "id": "39c69544"
      },
      "outputs": [],
      "source": [
        "from pymongo import MongoClient, errors\n",
        "from pymongo.server_api import ServerApi\n",
        "from pymongo.collection import Collection\n",
        "\n",
        "class MongoDBWriter:\n",
        "    \"\"\"MongoDB writer with error handling and relationship management\"\"\"\n",
        "\n",
        "    def __init__(self, connection_string=MONGODB_URI, database_name=DATABASE_NAME):\n",
        "        \"\"\"\n",
        "        Initialize MongoDB connection\n",
        "\n",
        "        Args:\n",
        "            connection_string: MongoDB connection URI (Atlas or local)\n",
        "            database_name: Database name to use\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Create client with server API version\n",
        "            self.client = MongoClient(\n",
        "                connection_string,\n",
        "                server_api=ServerApi('1'),\n",
        "                serverSelectionTimeoutMS=5000\n",
        "            )\n",
        "\n",
        "            # Test connection with ping\n",
        "            self.client.admin.command('ping')\n",
        "            logger.info(\"âœ… Pinged deployment - successfully connected to MongoDB!\")\n",
        "\n",
        "            self.db = self.client[database_name]\n",
        "\n",
        "            # Collections\n",
        "            self.institutions = self.db.institutions\n",
        "            self.programs = self.db.programs\n",
        "            self.raw_documents = self.db.raw_documents\n",
        "            self.extraction_logs = self.db.extraction_logs\n",
        "\n",
        "            # Create indexes\n",
        "            self._create_indexes()\n",
        "\n",
        "            logger.info(f\"âœ… Connected to MongoDB: {database_name}\")\n",
        "\n",
        "        except errors.ServerSelectionTimeoutError:\n",
        "            logger.error(\"âŒ Could not connect to MongoDB\")\n",
        "            logger.error(\"   Check your connection string and network access\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ MongoDB connection error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _create_indexes(self):\n",
        "        \"\"\"Create database indexes for performance\"\"\"\n",
        "        # Institution indexes\n",
        "        self.institutions.create_index(\"name\")\n",
        "        self.institutions.create_index(\"institution_code\", unique=True, sparse=True)\n",
        "\n",
        "        # Program indexes\n",
        "        self.programs.create_index(\"institution_id\")\n",
        "        self.programs.create_index(\"name\")\n",
        "        self.programs.create_index(\"level\")\n",
        "\n",
        "        # People indexes\n",
        "        logger.info(\"Database indexes created\")\n",
        "\n",
        "    def write_extraction(self, result: ExtractionResult) -> dict:\n",
        "        \"\"\"\n",
        "        Write complete extraction result to MongoDB\n",
        "\n",
        "        Args:\n",
        "            result: Validated ExtractionResult\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with inserted IDs\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Insert institution\n",
        "            institution_data = result.institution.model_dump() # Changed .dict() to .model_dump()\n",
        "            institution_data['inserted_at'] = datetime.now()\n",
        "\n",
        "            inst_result = self.institutions.insert_one(institution_data)\n",
        "            institution_id = inst_result.inserted_id\n",
        "\n",
        "            logger.info(f\"âœ… Inserted institution: {institution_id}\")\n",
        "\n",
        "            # Insert programs\n",
        "            program_ids = []\n",
        "            for program in result.programs:\n",
        "                program_data = program.model_dump() # Changed .dict() to .model_dump()\n",
        "                program_data['institution_id'] = institution_id\n",
        "                program_data['inserted_at'] = datetime.now()\n",
        "\n",
        "                prog_result = self.programs.insert_one(program_data)\n",
        "                program_ids.append(prog_result.inserted_id)\n",
        "\n",
        "            logger.info(f\"âœ… Inserted {len(program_ids)} programs\")\n",
        "\n",
        "            raw_doc = {\n",
        "                'institution_id': institution_id,\n",
        "                'raw_text': result.raw_text,\n",
        "                'source_file': result.source_file,\n",
        "                'extraction_timestamp': result.extraction_timestamp,\n",
        "                'inserted_at': datetime.now()\n",
        "            }\n",
        "            raw_result = self.raw_documents.insert_one(raw_doc)\n",
        "\n",
        "            # Log extraction\n",
        "            log_entry = {\n",
        "                'institution_id': institution_id,\n",
        "                'source_file': result.source_file,\n",
        "                'programs_count': len(result.programs),\n",
        "                'institution_confidence': result.institution.confidence_score,\n",
        "                'avg_program_confidence': sum(p.confidence_score for p in result.programs) / len(result.programs) if result.programs else 0,\n",
        "                'timestamp': datetime.now(),\n",
        "                'status': 'success'\n",
        "                }\n",
        "            self.extraction_logs.insert_one(log_entry)\n",
        "\n",
        "            return {\n",
        "                'institution_id': institution_id,\n",
        "                'program_ids': program_ids,\n",
        "                'raw_document_id': raw_result.inserted_id\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"MongoDB write failed: {e}\")\n",
        "\n",
        "            # Log failure\n",
        "            log_entry = {\n",
        "                'source_file': result.source_file if hasattr(result, 'source_file') else None,\n",
        "                'error': str(e),\n",
        "                'timestamp': datetime.now(),\n",
        "                'status': 'failed'\n",
        "            }\n",
        "            self.extraction_logs.insert_one(log_entry)\n",
        "\n",
        "            raise\n",
        "\n",
        "    def get_statistics(self) -> dict:\n",
        "        \"\"\"Get database statistics\"\"\"\n",
        "        return {\n",
        "            'institutions': self.institutions.count_documents({}),\n",
        "            'programs': self.programs.count_documents({}),\n",
        "            'raw_documents': self.raw_documents.count_documents({}),\n",
        "            'extraction_logs': self.extraction_logs.count_documents({})\n",
        "        }\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close MongoDB connection\"\"\"\n",
        "        self.client.close()\n",
        "        logger.info(\"MongoDB connection closed\")\n",
        "\n",
        "# Initialize MongoDB writer with Atlas connection\n",
        "mongo_writer = MongoDBWriter()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a089bc9",
      "metadata": {
        "id": "2a089bc9"
      },
      "source": [
        "## ðŸ“ Step 10: Load and Process Text Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b0b89369",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0b89369",
        "outputId": "fcb24360-0e1a-4b62-b82a-e219f088a2f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… File loader initialized\n",
            "   Found 20 files to process\n",
            "\n",
            "   Sample files:\n",
            "   - www_bci_lk_programmes_undergraduate-programmes.txt\n",
            "   - www_bci_lk_academic-senate.txt\n",
            "   - www_bci_lk_course_bachelor-of-science-honours-in-computer-science.txt\n",
            "   - www_bci_lk_course_advanced-certificate-in-ict.txt\n",
            "   - www_bci_lk_course_certificate-in-counselling-programme.txt\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "class FileLoader:\n",
        "    \"\"\"Load and preprocess text files\"\"\"\n",
        "\n",
        "    def __init__(self, input_dir: Path):\n",
        "        self.input_dir = Path(input_dir)\n",
        "\n",
        "    def get_all_files(self, pattern=\"*.txt\") -> List[Path]:\n",
        "        \"\"\"Get all text files from input directory\"\"\"\n",
        "        files = list(self.input_dir.glob(pattern))\n",
        "        logger.info(f\"Found {len(files)} files in {self.input_dir}\")\n",
        "        return files\n",
        "\n",
        "    def load_file(self, file_path: Path) -> str:\n",
        "        \"\"\"Load and preprocess a single file\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Basic preprocessing\n",
        "            content = self._preprocess_text(content)\n",
        "\n",
        "            logger.info(f\"Loaded {file_path.name}: {len(content)} chars\")\n",
        "            return content\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load {file_path}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        # Remove excessive whitespace\n",
        "        text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "        text = re.sub(r' {2,}', ' ', text)\n",
        "\n",
        "        # Remove non-printable characters\n",
        "        text = ''.join(char for char in text if char.isprintable() or char in '\\n\\t')\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "# Initialize file loader\n",
        "file_loader = FileLoader(INPUT_DIR)\n",
        "\n",
        "# Get all files\n",
        "all_files = file_loader.get_all_files()\n",
        "print(f\"âœ… File loader initialized\")\n",
        "print(f\"   Found {len(all_files)} files to process\")\n",
        "print(f\"\\n   Sample files:\")\n",
        "for file in all_files[:5]:\n",
        "    print(f\"   - {file.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7af9468",
      "metadata": {
        "id": "b7af9468"
      },
      "source": [
        "## ðŸš€ Step 11: Run Extraction Pipeline on Single File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "231aa29f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "231aa29f",
        "outputId": "dfbdbb1c-ebc6-45ef-924c-f3bc1f652f7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Extraction pipeline ready\n"
          ]
        }
      ],
      "source": [
        "class ExtractionPipeline:\n",
        "    \"\"\"Master pipeline orchestrator\"\"\"\n",
        "\n",
        "    def __init__(self, ollama_client, chunker, validator, scorer, mongo_writer=None):\n",
        "        self.ollama_client = ollama_client\n",
        "        self.chunker = chunker\n",
        "        self.validator = validator\n",
        "        self.scorer = scorer\n",
        "        self.mongo_writer = mongo_writer\n",
        "\n",
        "    def process_file(self, file_path: Path, save_to_db=True) -> Optional[ExtractionResult]:\n",
        "        \"\"\"\n",
        "        Process a single file through the complete pipeline\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to input file\n",
        "            save_to_db: Whether to save results to MongoDB\n",
        "\n",
        "        Returns:\n",
        "            ExtractionResult or None if processing fails\n",
        "        \"\"\"\n",
        "        logger.info(f\"\\n{'='*60}\")\n",
        "        logger.info(f\"Processing: {file_path.name}\")\n",
        "        logger.info(f\"{'='*60}\")\n",
        "\n",
        "        try:\n",
        "            # Step 1: Load file\n",
        "            text = file_loader.load_file(file_path)\n",
        "\n",
        "            # Step 2: Chunk text\n",
        "            chunks = self.chunker.chunk_text(text)\n",
        "            logger.info(f\"Split into {len(chunks)} chunks\")\n",
        "\n",
        "            # Step 3: Process each chunk and merge results\n",
        "            all_programs = []\n",
        "            institution_data = None\n",
        "\n",
        "            for i, chunk in enumerate(chunks, 1):\n",
        "                logger.info(f\"Processing chunk {i}/{len(chunks)}\")\n",
        "\n",
        "                # Build prompt\n",
        "                prompt = SYSTEM_PROMPT + USER_PROMPT_TEMPLATE.format(content=chunk)\n",
        "\n",
        "                # Call LLM\n",
        "                try:\n",
        "                    llm_response = self.ollama_client.call(prompt)\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"LLM call failed for chunk {i}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                # Extract data from chunk\n",
        "                if 'institution' in llm_response and institution_data is None:\n",
        "                    institution_data = llm_response['institution']\n",
        "\n",
        "                if 'programs' in llm_response:\n",
        "                    all_programs.extend(llm_response['programs'])\n",
        "\n",
        "            # Step 4: Merge all data\n",
        "            if not institution_data:\n",
        "                logger.error(\"No institution data extracted\")\n",
        "                return None\n",
        "\n",
        "            merged_data = {\n",
        "                'institution': institution_data,\n",
        "                'programs': all_programs,\n",
        "                'raw_text': text[:5000]  # Store first 5000 chars for audit\n",
        "            }\n",
        "\n",
        "            # Step 5: Validate\n",
        "            result = DataValidator.validate_extraction(merged_data, str(file_path.name)) # Calling static method directly on class\n",
        "\n",
        "            # Step 6: Normalize confidence scores\n",
        "            result = self.scorer.normalize_confidence(result)\n",
        "\n",
        "            # Step 7: Calculate overall quality\n",
        "            quality_score = self.scorer.get_overall_quality_score(result)\n",
        "            logger.info(f\"Overall quality score: {quality_score:.2f}\")\n",
        "\n",
        "            # Step 8: Save to MongoDB\n",
        "            if save_to_db and self.mongo_writer:\n",
        "                inserted_ids = self.mongo_writer.write_extraction(result)\n",
        "                logger.info(f\"Saved to MongoDB: {inserted_ids['institution_id']}\")\n",
        "\n",
        "            # Step 9: Save JSON output\n",
        "            self._save_json_output(result, file_path)\n",
        "\n",
        "            logger.info(f\"âœ… Successfully processed {file_path.name}\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ Pipeline failed for {file_path.name}: {e}\")\n",
        "            logger.error(traceback.format_exc())\n",
        "            return None\n",
        "\n",
        "    def _save_json_output(self, result: ExtractionResult, file_path: Path):\n",
        "        \"\"\"Save extraction result as JSON file\"\"\"\n",
        "        output_file = OUTPUT_DIR / f\"{file_path.stem}_extracted.json\"\n",
        "\n",
        "        # Convert to dict and handle datetime serialization\n",
        "        result_dict = result.model_dump() # Changed .dict() to .model_dump()\n",
        "        result_dict['extraction_timestamp'] = result.extraction_timestamp.isoformat()\n",
        "\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(result_dict, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        logger.info(f\"JSON output saved: {output_file.name}\")\n",
        "\n",
        "# Initialize pipeline\n",
        "pipeline = ExtractionPipeline(\n",
        "    ollama_client=ollama_client,\n",
        "    chunker=chunker,\n",
        "    validator=validator,\n",
        "    scorer=scorer,\n",
        "    mongo_writer=mongo_writer\n",
        ")\n",
        "\n",
        "print(\"âœ… Extraction pipeline ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "99a8e6c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99a8e6c8",
        "outputId": "310f3d6d-eda0-4b68-e7d8-bef914a36849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ§ª Testing pipeline on: www_bci_lk_programmes_undergraduate-programmes.txt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:âŒ Pipeline failed for www_bci_lk_programmes_undergraduate-programmes.txt: 'function' object has no attribute 'validate_extraction'\n",
            "ERROR:__main__:Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2783511999.py\", line 70, in process_file\n",
            "    result = self.validator.validate_extraction(merged_data, str(file_path.name))\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'function' object has no attribute 'validate_extraction'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test pipeline on a single file\n",
        "if len(all_files) > 0:\n",
        "    test_file = all_files[0]  # Process first file\n",
        "    print(f\"\\nðŸ§ª Testing pipeline on: {test_file.name}\\n\")\n",
        "\n",
        "    result = pipeline.process_file(test_file, save_to_db=True)\n",
        "\n",
        "    if result:\n",
        "        print(f\"\\nâœ… Test completed successfully!\")\n",
        "        print(f\"\\nðŸ“Š Extraction Summary:\")\n",
        "        print(f\"   Institution: {result.institution.name}\")\n",
        "        print(f\"   Confidence: {result.institution.confidence_score}\")\n",
        "        print(f\"   Programs extracted: {len(result.programs)}\")\n",
        "        if result.programs:\n",
        "            print(f\"\\n   Sample programs:\")\n",
        "            for prog in result.programs[:3]:\n",
        "                print(f\"   - {prog.name} (confidence: {prog.confidence_score})\")\n",
        "else:\n",
        "    print(\"âŒ No files found to process\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3558195a",
        "outputId": "bd293d5b-b13b-4016-ac79-f4047a3adf18"
      },
      "source": [
        "# Test pipeline on a single file\n",
        "if len(all_files) > 0:\n",
        "    test_file = all_files[0]  # Process first file\n",
        "    print(f\"\\nâ„¹ï¸ Testing pipeline on: {test_file.name}\\n\")\n",
        "\n",
        "    result = pipeline.process_file(test_file, save_to_db=True)\n",
        "\n",
        "    if result:\n",
        "        print(f\"\\nâœ… Test completed successfully!\")\n",
        "        print(f\"\\nðŸ“Š Extraction Summary:\")\n",
        "        print(f\"   Institution: {result.institution.name}\")\n",
        "        print(f\"   Confidence: {result.institution.confidence_score}\")\n",
        "        print(f\"   Programs extracted: {len(result.programs)}\")\n",
        "        if result.programs:\n",
        "            print(f\"\\n   Sample programs:\")\n",
        "            for prog in result.programs[:3]:\n",
        "                print(f\"   - {prog.name} (confidence: {prog.confidence_score})\")\n",
        "else:\n",
        "    print(\"âŒ No files found to process\")"
      ],
      "id": "3558195a",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â„¹ï¸ Testing pipeline on: www_bci_lk_programmes_undergraduate-programmes.txt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:MongoDB write failed: E11000 duplicate key error collection: edu_platform.institutions index: institution_code_1 dup key: { institution_code: null }, full error: {'index': 0, 'code': 11000, 'errmsg': 'E11000 duplicate key error collection: edu_platform.institutions index: institution_code_1 dup key: { institution_code: null }', 'keyPattern': {'institution_code': 1}, 'keyValue': {'institution_code': None}}\n",
            "ERROR:__main__:âŒ Pipeline failed for www_bci_lk_programmes_undergraduate-programmes.txt: E11000 duplicate key error collection: edu_platform.institutions index: institution_code_1 dup key: { institution_code: null }, full error: {'index': 0, 'code': 11000, 'errmsg': 'E11000 duplicate key error collection: edu_platform.institutions index: institution_code_1 dup key: { institution_code: null }', 'keyPattern': {'institution_code': 1}, 'keyValue': {'institution_code': None}}\n",
            "ERROR:__main__:Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2626884717.py\", line 81, in process_file\n",
            "    inserted_ids = self.mongo_writer.write_extraction(result)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1877580645.py\", line 78, in write_extraction\n",
            "    inst_result = self.institutions.insert_one(institution_data)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pymongo/collection.py\", line 705, in insert_one\n",
            "    self._insert(document,\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pymongo/collection.py\", line 620, in _insert\n",
            "    return self._insert_one(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pymongo/collection.py\", line 609, in _insert_one\n",
            "    self.__database.client._retryable_write(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pymongo/mongo_client.py\", line 1552, in _retryable_write\n",
            "    return self._retry_with_session(retryable, func, s, None)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pymongo/mongo_client.py\", line 1438, in _retry_with_session\n",
            "    return self._retry_internal(retryable, func, session, bulk)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pymongo/mongo_client.py\", line 1470, in _retry_internal\n",
            "    return func(session, sock_info, retryable)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pymongo/collection.py\", line 607, in _insert_command\n",
            "    _check_write_command_response(result)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pymongo/helpers.py\", line 229, in _check_write_command_response\n",
            "    _raise_last_write_error(write_errors)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pymongo/helpers.py\", line 210, in _raise_last_write_error\n",
            "    raise DuplicateKeyError(error.get(\"errmsg\"), 11000, error)\n",
            "pymongo.errors.DuplicateKeyError: E11000 duplicate key error collection: edu_platform.institutions index: institution_code_1 dup key: { institution_code: null }, full error: {'index': 0, 'code': 11000, 'errmsg': 'E11000 duplicate key error collection: edu_platform.institutions index: institution_code_1 dup key: { institution_code: null }', 'keyPattern': {'institution_code': 1}, 'keyValue': {'institution_code': None}}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e83bc163",
        "outputId": "39db57c7-e0be-49fc-bae3-59f4ba96433d"
      },
      "source": [
        "# Test pipeline on a single file\n",
        "if len(all_files) > 0:\n",
        "    test_file = all_files[0]  # Process first file\n",
        "    print(f\"\\nâ„¹ï¸ Testing pipeline on: {test_file.name}\\n\")\n",
        "\n",
        "    result = pipeline.process_file(test_file, save_to_db=True)\n",
        "\n",
        "    if result:\n",
        "        print(f\"\\nâœ… Test completed successfully!\")\n",
        "        print(f\"\\nðŸ“Š Extraction Summary:\")\n",
        "        print(f\"   Institution: {result.institution.name}\")\n",
        "        print(f\"   Confidence: {result.institution.confidence_score}\")\n",
        "        print(f\"   Programs extracted: {len(result.programs)}\")\n",
        "        if result.programs:\n",
        "            print(f\"\\n   Sample programs:\")\n",
        "            for prog in result.programs[:3]:\n",
        "                print(f\"   - {prog.name} (confidence: {prog.confidence_score})\")\n",
        "else:\n",
        "    print(\"âŒ No files found to process\")"
      ],
      "id": "e83bc163",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â„¹ï¸ Testing pipeline on: www_bci_lk_programmes_undergraduate-programmes.txt\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Schema validation failed:\n",
            "  - institution.country: Input should be a valid string\n",
            "ERROR:__main__:âŒ Pipeline failed for www_bci_lk_programmes_undergraduate-programmes.txt: Schema validation failed:\n",
            "  - institution.country: Input should be a valid string\n",
            "ERROR:__main__:Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2074822042.py\", line 27, in validate_extraction\n",
            "    result = ExtractionResult(**data)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pydantic/main.py\", line 250, in __init__\n",
            "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "pydantic_core._pydantic_core.ValidationError: 1 validation error for ExtractionResult\n",
            "institution.country\n",
            "  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n",
            "    For further information visit https://errors.pydantic.dev/2.12/v/string_type\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2626884717.py\", line 70, in process_file\n",
            "    result = DataValidator.validate_extraction(merged_data, str(file_path.name)) # Calling static method directly on class\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2074822042.py\", line 44, in validate_extraction\n",
            "    raise ValueError(error_msg)\n",
            "ValueError: Schema validation failed:\n",
            "  - institution.country: Input should be a valid string\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a5a23b0",
      "metadata": {
        "id": "0a5a23b0"
      },
      "source": [
        "## ðŸ”„ Step 12: Batch Process All Institution Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffc765e",
      "metadata": {
        "id": "2ffc765e"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "class BatchProcessor:\n",
        "    \"\"\"Batch processing with retry logic and error tracking\"\"\"\n",
        "\n",
        "    def __init__(self, pipeline, max_retries=2, retry_delay=5):\n",
        "        self.pipeline = pipeline\n",
        "        self.max_retries = max_retries\n",
        "        self.retry_delay = retry_delay\n",
        "\n",
        "        self.results = {\n",
        "            'success': [],\n",
        "            'failed': [],\n",
        "            'retry_queue': []\n",
        "        }\n",
        "\n",
        "    def process_all_files(self, files: List[Path], save_to_db=True):\n",
        "        \"\"\"\n",
        "        Process all files with error handling and retry logic\n",
        "\n",
        "        Args:\n",
        "            files: List of file paths to process\n",
        "            save_to_db: Whether to save to MongoDB\n",
        "        \"\"\"\n",
        "        total_files = len(files)\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"BATCH PROCESSING: {total_files} files\")\n",
        "        print(f\"{'='*60}\\n\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for idx, file_path in enumerate(files, 1):\n",
        "            print(f\"\\n[{idx}/{total_files}] Processing: {file_path.name}\")\n",
        "\n",
        "            # Attempt processing with retries\n",
        "            success = self._process_with_retry(file_path, save_to_db)\n",
        "\n",
        "            if success:\n",
        "                self.results['success'].append(file_path.name)\n",
        "                print(f\"âœ… Success\")\n",
        "            else:\n",
        "                self.results['failed'].append(file_path.name)\n",
        "                print(f\"âŒ Failed after {self.max_retries} retries\")\n",
        "\n",
        "            # Progress update\n",
        "            success_rate = len(self.results['success']) / idx * 100\n",
        "            print(f\"Progress: {idx}/{total_files} ({success_rate:.1f}% success rate)\")\n",
        "\n",
        "        # Final summary\n",
        "        elapsed_time = time.time() - start_time\n",
        "        self._print_summary(elapsed_time)\n",
        "\n",
        "    def _process_with_retry(self, file_path: Path, save_to_db: bool) -> bool:\n",
        "        \"\"\"Process file with retry logic\"\"\"\n",
        "        for attempt in range(self.max_retries + 1):\n",
        "            try:\n",
        "                result = self.pipeline.process_file(file_path, save_to_db)\n",
        "\n",
        "                if result:\n",
        "                    return True\n",
        "\n",
        "                if attempt < self.max_retries:\n",
        "                    logger.warning(f\"Retry {attempt + 1}/{self.max_retries} for {file_path.name}\")\n",
        "                    time.sleep(self.retry_delay)\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt < self.max_retries:\n",
        "                    time.sleep(self.retry_delay)\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _print_summary(self, elapsed_time: float):\n",
        "        \"\"\"Print processing summary\"\"\"\n",
        "        total = len(self.results['success']) + len(self.results['failed'])\n",
        "        success_count = len(self.results['success'])\n",
        "        failed_count = len(self.results['failed'])\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"BATCH PROCESSING COMPLETE\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"\\nðŸ“Š Summary:\")\n",
        "        print(f\"   Total files: {total}\")\n",
        "        print(f\"   âœ… Successful: {success_count} ({success_count/total*100:.1f}%)\")\n",
        "        print(f\"   âŒ Failed: {failed_count} ({failed_count/total*100:.1f}%)\")\n",
        "        print(f\"   â±ï¸  Time elapsed: {elapsed_time:.1f}s\")\n",
        "        print(f\"   âš¡ Avg time/file: {elapsed_time/total:.1f}s\")\n",
        "\n",
        "        if self.results['failed']:\n",
        "            print(f\"\\nâŒ Failed files:\")\n",
        "            for filename in self.results['failed']:\n",
        "                print(f\"   - {filename}\")\n",
        "\n",
        "        # Save summary to file\n",
        "        summary_file = LOGS_DIR / f\"batch_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        with open(summary_file, 'w') as f:\n",
        "            json.dump({\n",
        "                'total': total,\n",
        "                'success': success_count,\n",
        "                'failed': failed_count,\n",
        "                'elapsed_time': elapsed_time,\n",
        "                'success_files': self.results['success'],\n",
        "                'failed_files': self.results['failed']\n",
        "            }, f, indent=2)\n",
        "\n",
        "        print(f\"\\nðŸ“„ Summary saved to: {summary_file.name}\")\n",
        "\n",
        "# Initialize batch processor\n",
        "batch_processor = BatchProcessor(pipeline, max_retries=2, retry_delay=3)\n",
        "\n",
        "print(\"âœ… Batch processor ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f20f125d",
      "metadata": {
        "id": "f20f125d"
      },
      "outputs": [],
      "source": [
        "# Run batch processing on all files\n",
        "# WARNING: This will process ALL files - may take a long time!\n",
        "# Uncomment the line below to process all files\n",
        "\n",
        "# batch_processor.process_all_files(all_files, save_to_db=True)\n",
        "\n",
        "print(\"âš ï¸  To process all files, uncomment the line above\")\n",
        "print(f\"   {len(all_files)} files ready to process\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1e55400",
      "metadata": {
        "id": "b1e55400"
      },
      "source": [
        "## âœ… Step 13: Validate and Display Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62ff5bc9",
      "metadata": {
        "id": "62ff5bc9"
      },
      "outputs": [],
      "source": [
        "# Check generated JSON outputs\n",
        "output_files = list(OUTPUT_DIR.glob(\"*_extracted.json\"))\n",
        "\n",
        "print(f\"ðŸ“Š Generated Outputs: {len(output_files)} files\\n\")\n",
        "\n",
        "if output_files:\n",
        "    # Display first extraction result\n",
        "    sample_file = output_files[0]\n",
        "    print(f\"Sample extraction: {sample_file.name}\\n\")\n",
        "\n",
        "    with open(sample_file, 'r') as f:\n",
        "        sample_data = json.load(f)\n",
        "\n",
        "    # Display institution info\n",
        "    print(\"ðŸ›ï¸  Institution:\")\n",
        "    inst = sample_data['institution']\n",
        "    print(f\"   Name: {inst['name']}\")\n",
        "    print(f\"   Type: {', '.join(inst['type'])}\")\n",
        "    print(f\"   Country: {inst['country']}\")\n",
        "    print(f\"   Confidence: {inst['confidence_score']}\")\n",
        "\n",
        "    # Display programs\n",
        "    if sample_data['programs']:\n",
        "        print(f\"\\nðŸ“š Programs ({len(sample_data['programs'])}):\")\n",
        "        for i, prog in enumerate(sample_data['programs'][:5], 1):\n",
        "            print(f\"   {i}. {prog['name']}\")\n",
        "            print(f\"      Level: {prog.get('level', 'N/A')}\")\n",
        "            print(f\"      Confidence: {prog['confidence_score']}\")\n",
        "\n",
        "    print(f\"\\nðŸ“… Extraction timestamp: {sample_data['extraction_timestamp']}\")\n",
        "else:\n",
        "    print(\"âŒ No extraction results found. Run the pipeline first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d4d4356",
      "metadata": {
        "id": "4d4d4356"
      },
      "source": [
        "## ðŸ” Step 14: Query MongoDB Collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08d3969a",
      "metadata": {
        "id": "08d3969a"
      },
      "outputs": [],
      "source": [
        "if mongo_writer:\n",
        "    print(\"ðŸ“Š MongoDB Database Statistics\\n\")\n",
        "\n",
        "    # Get statistics\n",
        "    stats = mongo_writer.get_statistics()\n",
        "    print(f\"Collections:\")\n",
        "    for collection, count in stats.items():\n",
        "        print(f\"   {collection}: {count} documents\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # Query institutions\n",
        "    print(\"\\nðŸ›ï¸  Institutions:\")\n",
        "    institutions = list(mongo_writer.institutions.find().limit(5))\n",
        "    for inst in institutions:\n",
        "        print(f\"\\n   Name: {inst['name']}\")\n",
        "        print(f\"   Confidence: {inst['confidence_score']}\")\n",
        "        if 'website' in inst and inst['website']:\n",
        "            print(f\"   Website: {inst['website']}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # Query programs with aggregation\n",
        "    print(\"\\nðŸ“š Programs by Confidence Level:\")\n",
        "    pipeline = [\n",
        "        {\n",
        "            '$bucket': {\n",
        "                'groupBy': '$confidence_score',\n",
        "                'boundaries': [0.0, 0.5, 0.7, 0.9, 1.0],\n",
        "                'default': 'Other',\n",
        "                'output': {\n",
        "                    'count': {'$sum': 1},\n",
        "                    'programs': {'$push': '$name'}\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    confidence_buckets = list(mongo_writer.programs.aggregate(pipeline))\n",
        "    for bucket in confidence_buckets:\n",
        "        range_str = f\"{bucket['_id']}-{bucket['_id']+0.2:.1f}\" if isinstance(bucket['_id'], float) else bucket['_id']\n",
        "        print(f\"   {range_str}: {bucket['count']} programs\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # Query high-confidence programs\n",
        "    print(\"\\nâ­ High Confidence Programs (>0.8):\")\n",
        "    high_conf_programs = list(mongo_writer.programs.find(\n",
        "        {'confidence_score': {'$gt': 0.8}}\n",
        "    ).limit(10))\n",
        "\n",
        "    for prog in high_conf_programs:\n",
        "        print(f\"   - {prog['name']} ({prog['confidence_score']})\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # Query extraction logs\n",
        "    print(\"\\nðŸ“ Recent Extraction Logs:\")\n",
        "    logs = list(mongo_writer.extraction_logs.find().sort('timestamp', -1).limit(5))\n",
        "\n",
        "    for log in logs:\n",
        "        status_icon = \"âœ…\" if log['status'] == 'success' else \"âŒ\"\n",
        "        print(f\"\\n   {status_icon} {log.get('source_file', 'Unknown')}\")\n",
        "        print(f\"      Status: {log['status']}\")\n",
        "        print(f\"      Timestamp: {log['timestamp']}\")\n",
        "        if 'programs_count' in log:\n",
        "            print(f\"      Programs: {log['programs_count']}\")\n",
        "        if 'error' in log:\n",
        "            print(f\"      Error: {log['error'][:100]}...\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "    # Advanced query: Programs with missing fields\n",
        "    print(\"\\nâš ï¸  Programs with Missing Duration Info:\")\n",
        "    missing_duration = mongo_writer.programs.count_documents({'duration': None})\n",
        "    print(f\"   Count: {missing_duration}\")\n",
        "\n",
        "    print(\"\\nâš ï¸  Programs with Missing Level Info:\")\n",
        "    missing_level = mongo_writer.programs.count_documents({'level': None})\n",
        "    print(f\"   Count: {missing_level}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ MongoDB not connected. Cannot query database.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b20c82f",
      "metadata": {
        "id": "0b20c82f"
      },
      "source": [
        "## ðŸŽ¯ Additional Features: Retry Queue & Data Quality Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a90cccb",
      "metadata": {
        "id": "3a90cccb"
      },
      "outputs": [],
      "source": [
        "class DataQualityAnalyzer:\n",
        "    \"\"\"Analyze data quality and generate metrics\"\"\"\n",
        "\n",
        "    def __init__(self, mongo_writer):\n",
        "        self.mongo_writer = mongo_writer\n",
        "\n",
        "    def generate_quality_report(self) -> dict:\n",
        "        \"\"\"Generate comprehensive data quality report\"\"\"\n",
        "        if not self.mongo_writer:\n",
        "            return {\"error\": \"MongoDB not connected\"}\n",
        "\n",
        "        report = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'institutions': {},\n",
        "            'programs': {},\n",
        "            'overall': {}\n",
        "        }\n",
        "\n",
        "        # Institution metrics\n",
        "        institutions = list(self.mongo_writer.institutions.find())\n",
        "        if institutions:\n",
        "            inst_scores = [i['confidence_score'] for i in institutions]\n",
        "            report['institutions'] = {\n",
        "                'total': len(institutions),\n",
        "                'avg_confidence': sum(inst_scores) / len(inst_scores),\n",
        "                'min_confidence': min(inst_scores),\n",
        "                'max_confidence': max(inst_scores),\n",
        "                'missing_website': sum(1 for i in institutions if not i.get('website')),\n",
        "                'missing_code': sum(1 for i in institutions if not i.get('institution_code'))\n",
        "            }\n",
        "\n",
        "        # Program metrics\n",
        "        programs = list(self.mongo_writer.programs.find())\n",
        "        if programs:\n",
        "            prog_scores = [p['confidence_score'] for p in programs]\n",
        "            report['programs'] = {\n",
        "                'total': len(programs),\n",
        "                'avg_confidence': sum(prog_scores) / len(prog_scores),\n",
        "                'min_confidence': min(prog_scores),\n",
        "                'max_confidence': max(prog_scores),\n",
        "                'missing_level': sum(1 for p in programs if not p.get('level')),\n",
        "                'missing_duration': sum(1 for p in programs if not p.get('duration')),\n",
        "                'missing_fees': sum(1 for p in programs if not p.get('fees')),\n",
        "                'missing_curriculum': sum(1 for p in programs if not p.get('curriculum_summary'))\n",
        "            }\n",
        "\n",
        "        # Overall quality score\n",
        "        all_scores = inst_scores + prog_scores if (institutions and programs) else []\n",
        "        if all_scores:\n",
        "            report['overall'] = {\n",
        "                'total_records': len(all_scores),\n",
        "                'avg_confidence': sum(all_scores) / len(all_scores),\n",
        "                'quality_grade': self._calculate_grade(sum(all_scores) / len(all_scores))\n",
        "            }\n",
        "\n",
        "        return report\n",
        "\n",
        "    def _calculate_grade(self, score: float) -> str:\n",
        "        \"\"\"Calculate quality grade from score\"\"\"\n",
        "        if score >= 0.9:\n",
        "            return 'A - Excellent'\n",
        "        elif score >= 0.8:\n",
        "            return 'B - Good'\n",
        "        elif score >= 0.7:\n",
        "            return 'C - Acceptable'\n",
        "        elif score >= 0.6:\n",
        "            return 'D - Poor'\n",
        "        else:\n",
        "            return 'F - Very Poor'\n",
        "\n",
        "    def print_report(self):\n",
        "        \"\"\"Print formatted quality report\"\"\"\n",
        "        report = self.generate_quality_report()\n",
        "\n",
        "        if 'error' in report:\n",
        "            print(f\"âŒ {report['error']}\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"ðŸ“Š DATA QUALITY REPORT\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        if 'institutions' in report and report['institutions']:\n",
        "            print(\"\\nðŸ›ï¸  Institutions:\")\n",
        "            inst = report['institutions']\n",
        "            print(f\"   Total: {inst['total']}\")\n",
        "            print(f\"   Avg Confidence: {inst['avg_confidence']:.3f}\")\n",
        "            print(f\"   Range: {inst['min_confidence']:.3f} - {inst['max_confidence']:.3f}\")\n",
        "            print(f\"   Missing Website: {inst['missing_website']} ({inst['missing_website']/inst['total']*100:.1f}%)\")\n",
        "            print(f\"   Missing Code: {inst['missing_code']} ({inst['missing_code']/inst['total']*100:.1f}%)\")\n",
        "\n",
        "        if 'programs' in report and report['programs']:\n",
        "            print(\"\\nðŸ“š Programs:\")\n",
        "            prog = report['programs']\n",
        "            print(f\"   Total: {prog['total']}\")\n",
        "            print(f\"   Avg Confidence: {prog['avg_confidence']:.3f}\")\n",
        "            print(f\"   Range: {prog['min_confidence']:.3f} - {prog['max_confidence']:.3f}\")\n",
        "            print(f\"   Missing Level: {prog['missing_level']} ({prog['missing_level']/prog['total']*100:.1f}%)\")\n",
        "            print(f\"   Missing Duration: {prog['missing_duration']} ({prog['missing_duration']/prog['total']*100:.1f}%)\")\n",
        "            print(f\"   Missing Fees: {prog['missing_fees']} ({prog['missing_fees']/prog['total']*100:.1f}%)\")\n",
        "            print(f\"   Missing Curriculum: {prog['missing_curriculum']} ({prog['missing_curriculum']/prog['total']*100:.1f}%)\")\n",
        "\n",
        "        if 'overall' in report and report['overall']:\n",
        "            print(\"\\nâ­ Overall Quality:\")\n",
        "            overall = report['overall']\n",
        "            print(f\"   Total Records: {overall['total_records']}\")\n",
        "            print(f\"   Avg Confidence: {overall['avg_confidence']:.3f}\")\n",
        "            print(f\"   Quality Grade: {overall['quality_grade']}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "        # Save report\n",
        "        report_file = LOGS_DIR / f\"quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        with open(report_file, 'w') as f:\n",
        "            json.dump(report, f, indent=2)\n",
        "        print(f\"\\nðŸ“„ Report saved to: {report_file.name}\\n\")\n",
        "\n",
        "# Initialize quality analyzer\n",
        "if mongo_writer:\n",
        "    quality_analyzer = DataQualityAnalyzer(mongo_writer)\n",
        "    print(\"âœ… Data quality analyzer ready\")\n",
        "else:\n",
        "    quality_analyzer = None\n",
        "    print(\"âš ï¸  Quality analyzer unavailable (MongoDB not connected)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd781ae",
      "metadata": {
        "id": "5cd781ae"
      },
      "outputs": [],
      "source": [
        "# Generate and display quality report\n",
        "if quality_analyzer:\n",
        "    quality_analyzer.print_report()\n",
        "else:\n",
        "    print(\"âŒ Cannot generate report - MongoDB not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6cbf009",
      "metadata": {
        "id": "b6cbf009"
      },
      "source": [
        "## ðŸ“ Usage Examples & Quick Reference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f008375",
      "metadata": {
        "id": "5f008375"
      },
      "source": [
        "### Quick Reference Guide\n",
        "\n",
        "#### 1ï¸âƒ£ Process a Single File\n",
        "```python\n",
        "file_path = Path(\"/path/to/file.txt\")\n",
        "result = pipeline.process_file(file_path, save_to_db=True)\n",
        "```\n",
        "\n",
        "#### 2ï¸âƒ£ Process All Files in Batch\n",
        "```python\n",
        "batch_processor.process_all_files(all_files, save_to_db=True)\n",
        "```\n",
        "\n",
        "#### 3ï¸âƒ£ Query MongoDB\n",
        "```python\n",
        "# Get all institutions\n",
        "institutions = list(mongo_writer.institutions.find())\n",
        "\n",
        "# Get high-confidence programs\n",
        "high_conf = list(mongo_writer.programs.find({'confidence_score': {'$gt': 0.8}}))\n",
        "\n",
        "# Get programs for specific institution\n",
        "programs = list(mongo_writer.programs.find({'institution_id': institution_id}))\n",
        "```\n",
        "\n",
        "#### 4ï¸âƒ£ Generate Quality Report\n",
        "```python\n",
        "quality_analyzer.print_report()\n",
        "```\n",
        "\n",
        "#### 5ï¸âƒ£ Export Data\n",
        "```python\n",
        "# Export to JSON\n",
        "import pandas as pd\n",
        "\n",
        "institutions_df = pd.DataFrame(list(mongo_writer.institutions.find()))\n",
        "institutions_df.to_json('institutions_export.json', orient='records', indent=2)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ Configuration Options\n",
        "\n",
        "**Ollama Model:**\n",
        "- Default: `llama3`\n",
        "- Change: `ollama_client = OllamaClient(model=\"mistral\")`\n",
        "\n",
        "**Chunking:**\n",
        "- Max chars: 3000 (default)\n",
        "- Overlap: 200 (default)\n",
        "- Adjust: `chunker = TextChunker(max_chars=5000, overlap=300)`\n",
        "\n",
        "**Confidence Thresholds:**\n",
        "- Low confidence: 0.6 (default)\n",
        "- Penalty factor: 0.9 (default)\n",
        "- Adjust: `scorer = ConfidenceScorer(low_confidence_threshold=0.7, penalty_factor=0.85)`\n",
        "\n",
        "**MongoDB:**\n",
        "- Default: `mongodb://localhost:27017`\n",
        "- Database: `edu_platform`\n",
        "- Change: `mongo_writer = MongoDBWriter(connection_string=\"mongodb://...\n",
        "\n",
        "\")`\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”§ Troubleshooting\n",
        "\n",
        "**Issue: Ollama not connecting**\n",
        "- Check if Ollama is running: `ps aux | grep ollama`\n",
        "- Start server: `ollama serve`\n",
        "- Pull model: `ollama pull llama3`\n",
        "\n",
        "**Issue: MongoDB connection failed**\n",
        "- Check if MongoDB is running: `ps aux | grep mongod`\n",
        "- Start MongoDB: `mongod` or `sudo systemctl start mongod`\n",
        "\n",
        "**Issue: Low extraction quality**\n",
        "- Increase chunk overlap for better context\n",
        "- Adjust temperature (lower = more deterministic)\n",
        "- Try different LLM model (mistral, llama3.1, etc.)\n",
        "\n",
        "**Issue: JSON parsing errors**\n",
        "- LLM may return markdown - cleaning is automated\n",
        "- Check prompt clarity\n",
        "- Reduce chunk size for better focus\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62a9a753",
      "metadata": {
        "id": "62a9a753"
      },
      "source": [
        "## ðŸŽ‰ Summary & Next Steps\n",
        "\n",
        "### âœ… What This Pipeline Does\n",
        "\n",
        "1. **Loads** raw text files from educational institutions\n",
        "2. **Chunks** large documents into manageable pieces\n",
        "3. **Extracts** structured data using Ollama LLM (LLaMA 3)\n",
        "4. **Validates** output against strict Pydantic schemas\n",
        "5. **Scores** confidence levels for data quality\n",
        "6. **Normalizes** scores based on completeness\n",
        "7. **Stores** in MongoDB with proper relationships\n",
        "8. **Audits** with logs and raw document storage\n",
        "9. **Analyzes** data quality with comprehensive metrics\n",
        "10. **Exports** results as JSON for downstream use\n",
        "\n",
        "### ðŸš€ Next Steps\n",
        "\n",
        "**Immediate:**\n",
        "- [ ] Ensure Ollama is installed and running with LLaMA 3 model\n",
        "- [ ] Start MongoDB server\n",
        "- [ ] Run test on single file to verify setup\n",
        "- [ ] Review extraction quality and adjust prompts if needed\n",
        "\n",
        "**Short-term:**\n",
        "- [ ] Process all files in batch mode\n",
        "- [ ] Review quality report and identify gaps\n",
        "- [ ] Fine-tune confidence thresholds\n",
        "- [ ] Create retry queue for failed extractions\n",
        "\n",
        "**Long-term:**\n",
        "- [ ] Implement vector embeddings for semantic search\n",
        "- [ ] Add schema evolution tracking\n",
        "- [ ] Build API layer for data access\n",
        "- [ ] Create data visualization dashboard\n",
        "- [ ] Implement incremental updates (delta processing)\n",
        "- [ ] Add multilingual support\n",
        "\n",
        "### ðŸ“š Resources\n",
        "\n",
        "- **Ollama Documentation:** https://ollama.ai/\n",
        "- **Pydantic Docs:** https://docs.pydantic.dev/\n",
        "- **MongoDB Manual:** https://docs.mongodb.com/\n",
        "- **LLaMA 3:** https://ai.meta.com/llama/\n",
        "\n",
        "---\n",
        "\n",
        "**Pipeline Status:** âœ… Ready to run\n",
        "**Version:** 1.0.0\n",
        "**Last Updated:** January 2026"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}