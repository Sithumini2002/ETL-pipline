{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# University Academic Web Crawler with Ollama\n",
        "\n",
        "This notebook implements an intelligent web crawler that uses **Ollama (Llama 3.2)** to filter and extract educational program data from university websites.\n",
        "\n",
        "**Goal**: Build a database of:\n",
        "- Educational courses & programs\n",
        "- Admission criteria & requirements\n",
        "- Certificates & diplomas\n",
        "- Academic pathways & progressions"
      ],
      "metadata": {
        "id": "title_cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Ollama"
      ],
      "metadata": {
        "id": "step1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "install_ollama"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Start Ollama Server"
      ],
      "metadata": {
        "id": "step2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "    subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "# Kill any existing Ollama processes\n",
        "!pkill -9 ollama\n",
        "time.sleep(2)\n",
        "\n",
        "# Start fresh Ollama server\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)\n",
        "print(\"âœ“ Ollama server started\")"
      ],
      "metadata": {
        "id": "start_server"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Pull Llama Model"
      ],
      "metadata": {
        "id": "step3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.2"
      ],
      "metadata": {
        "id": "pull_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Install Dependencies"
      ],
      "metadata": {
        "id": "step4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-ollama beautifulsoup4 lxml requests"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Import Libraries"
      ],
      "metadata": {
        "id": "step5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Set\n",
        "from urllib.parse import urljoin, urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "print(\"âœ“ All libraries imported successfully\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Initialize Ollama Model"
      ],
      "metadata": {
        "id": "step6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Llama model with connection retry\n",
        "import time\n",
        "\n",
        "max_retries = 3\n",
        "for attempt in range(max_retries):\n",
        "    try:\n",
        "        llm = OllamaLLM(model=\"llama3.2\", temperature=0)\n",
        "        # Test the connection\n",
        "        test = llm.invoke(\"Say OK\")\n",
        "        print(\"âœ“ Llama 3.2 model initialized successfully\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        if attempt < max_retries - 1:\n",
        "            print(f\"âš  Connection failed (attempt {attempt+1}/{max_retries}), retrying...\")\n",
        "            # Restart Ollama server\n",
        "            !pkill -9 ollama\n",
        "            time.sleep(2)\n",
        "            import threading\n",
        "            import subprocess\n",
        "            def run_ollama():\n",
        "                subprocess.Popen([\"ollama\", \"serve\"])\n",
        "            threading.Thread(target=run_ollama).start()\n",
        "            time.sleep(5)\n",
        "        else:\n",
        "            print(f\"âœ— Failed to initialize Ollama after {max_retries} attempts\")\n",
        "            print(\"Please manually run: ollama serve\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "init_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Define Utility Functions"
      ],
      "metadata": {
        "id": "step7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_url(url: str) -> str:\n",
        "    \"\"\"Remove fragments and queries, return canonical URL.\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    return parsed._replace(fragment=\"\", query=\"\").geturl()\n",
        "\n",
        "def fetch_page(url: str) -> Dict[str, Any]:\n",
        "    \"\"\"Fetch a page and return its content.\"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "    }\n",
        "    r = requests.get(url, timeout=30, headers=headers)\n",
        "    r.raise_for_status()\n",
        "    \n",
        "    soup = BeautifulSoup(r.text, \"lxml\")\n",
        "    text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "    \n",
        "    return {\n",
        "        \"url\": url,\n",
        "        \"status_code\": r.status_code,\n",
        "        \"html\": r.text,\n",
        "        \"text\": text\n",
        "    }\n",
        "\n",
        "def extract_internal_links_with_anchor(html: str, base_url: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"Extract internal links and their anchor text from HTML.\"\"\"\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    base_domain = urlparse(base_url).netloc\n",
        "    results = []\n",
        "    \n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        full_url = urljoin(base_url, a[\"href\"])\n",
        "        parsed = urlparse(full_url)\n",
        "        \n",
        "        if parsed.netloc == base_domain and parsed.scheme in (\"http\", \"https\"):\n",
        "            clean_url = parsed._replace(fragment=\"\", query=\"\").geturl()\n",
        "            anchor = a.get_text(strip=True)\n",
        "            results.append({\"url\": clean_url, \"anchor\": anchor})\n",
        "    \n",
        "    # Deduplicate by URL\n",
        "    seen = {}\n",
        "    for r in results:\n",
        "        seen[r[\"url\"]] = r[\"anchor\"]\n",
        "    \n",
        "    return [{\"url\": u, \"anchor\": a} for u, a in seen.items()]\n",
        "\n",
        "print(\"âœ“ Utility functions defined\")"
      ],
      "metadata": {
        "id": "utils"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: AI-Powered Link Filtering"
      ],
      "metadata": {
        "id": "step8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_relevant_urls_with_ai(links: List[Dict[str, str]], llm, batch_size: int = 15) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Use Ollama/Llama to intelligently filter links based on educational content relevance.\n",
        "    \"\"\"\n",
        "    if not links:\n",
        "        return []\n",
        "    \n",
        "    filtered = []\n",
        "    total_batches = (len(links) + batch_size - 1) // batch_size\n",
        "    \n",
        "    for batch_num, i in enumerate(range(0, len(links), batch_size), 1):\n",
        "        batch = links[i:i + batch_size]\n",
        "        \n",
        "        # Format links for AI analysis\n",
        "        links_text = \"\\n\".join([\n",
        "            f\"{idx}. URL: {link['url']}\\n   Anchor: {link['anchor']}\"\n",
        "            for idx, link in enumerate(batch, 1)\n",
        "        ])\n",
        "        \n",
        "        prompt = f\"\"\"You are filtering web links for an educational database crawler.\n",
        "\n",
        "INCLUDE links about:\n",
        "- Courses, programs, degrees, diplomas, certificates\n",
        "- Admissions, eligibility, requirements, applications\n",
        "- Curriculum, syllabus, course structures, pathways\n",
        "- Academic departments with program listings\n",
        "\n",
        "EXCLUDE links about:\n",
        "- Staff profiles, news, events, research papers\n",
        "- Libraries, IT services, student portals\n",
        "- Administration, governance, about pages\n",
        "- Login pages, downloads, galleries\n",
        "\n",
        "Links to analyze:\n",
        "{links_text}\n",
        "\n",
        "Return ONLY a JSON array of relevant link numbers, e.g., [1, 3, 5]\n",
        "If none are relevant, return []\n",
        "\n",
        "JSON:\"\"\"\n",
        "        \n",
        "        try:\n",
        "            response = llm.invoke(prompt)\n",
        "            # Extract JSON from response\n",
        "            response = response.strip()\n",
        "            if response.startswith('[') and response.endswith(']'):\n",
        "                relevant_indices = json.loads(response)\n",
        "            else:\n",
        "                # Try to find JSON in response\n",
        "                import re\n",
        "                match = re.search(r'\\[.*?\\]', response)\n",
        "                if match:\n",
        "                    relevant_indices = json.loads(match.group())\n",
        "                else:\n",
        "                    relevant_indices = []\n",
        "            \n",
        "            # Add relevant links\n",
        "            for idx in relevant_indices:\n",
        "                if 1 <= idx <= len(batch):\n",
        "                    filtered.append(batch[idx - 1])\n",
        "            \n",
        "            print(f\"    [Filter Batch {batch_num}/{total_batches}] {len(relevant_indices)}/{len(batch)} links relevant\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"    âš  AI filter error: {e}\")\n",
        "            # Fallback to simple keyword filtering\n",
        "            for link in batch:\n",
        "                text = (link['url'] + ' ' + link['anchor']).lower()\n",
        "                if any(kw in text for kw in ['course', 'program', 'degree', 'admission', 'curriculum']):\n",
        "                    if not any(kw in text for kw in ['news', 'event', 'staff', 'research', 'login']):\n",
        "                        filtered.append(link)\n",
        "        \n",
        "        # Rate limiting\n",
        "        if batch_num < total_batches:\n",
        "            time.sleep(1)\n",
        "    \n",
        "    return filtered\n",
        "\n",
        "print(\"âœ“ AI link filter defined\")"
      ],
      "metadata": {
        "id": "ai_filter"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: AI-Powered Page Expansion Decision"
      ],
      "metadata": {
        "id": "step9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def should_expand_page(page_content: str, url: str, llm) -> tuple[bool, str]:\n",
        "    \"\"\"\n",
        "    Use Ollama/Llama to decide if page should be expanded for further crawling.\n",
        "    Returns (should_expand, reason)\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"Analyze if this webpage contains educational program information worth crawling deeper.\n",
        "\n",
        "URL: {url}\n",
        "\n",
        "EXPAND if page contains:\n",
        "- Course/program listings or descriptions\n",
        "- Admission requirements or eligibility criteria\n",
        "- Curriculum details or academic pathways\n",
        "- Certificate/diploma program information\n",
        "\n",
        "SKIP if page is about:\n",
        "- Staff profiles, news, events, research\n",
        "- Administrative info, IT services, libraries\n",
        "- About us, history, governance, rankings\n",
        "\n",
        "Page content (first 3000 chars):\n",
        "{page_content[:3000]}\n",
        "\n",
        "Return ONLY a JSON object: {{\"expand\": true/false, \"reason\": \"brief explanation\"}}\n",
        "\n",
        "JSON:\"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        # Try to parse JSON from response\n",
        "        import re\n",
        "        response = response.strip()\n",
        "        \n",
        "        # Try direct JSON parse\n",
        "        if response.startswith('{'):\n",
        "            result = json.loads(response)\n",
        "        else:\n",
        "            # Extract JSON from markdown or text\n",
        "            match = re.search(r'\\{.*?\\}', response, re.DOTALL)\n",
        "            if match:\n",
        "                result = json.loads(match.group())\n",
        "            else:\n",
        "                return False, \"Failed to parse AI response\"\n",
        "        \n",
        "        should_expand = bool(result.get(\"expand\", False))\n",
        "        reason = result.get(\"reason\", \"No reason provided\")\n",
        "        return should_expand, reason\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"    âš  AI expansion error: {e}\")\n",
        "        return False, f\"Error: {str(e)}\"\n",
        "\n",
        "print(\"âœ“ Page expansion function defined\")"
      ],
      "metadata": {
        "id": "page_expand"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 10: Academic Crawler Class"
      ],
      "metadata": {
        "id": "step10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AcademicCrawler:\n",
        "    \"\"\"\n",
        "    Recursive academic web crawler with Ollama/Llama-based filtering.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm, output_dir: str, max_depth: int = 3, rate_limit: int = 1):\n",
        "        self.llm = llm\n",
        "        self.output_dir = output_dir\n",
        "        self.max_depth = max_depth\n",
        "        self.rate_limit = rate_limit\n",
        "        self.visited: Set[str] = set()\n",
        "    \n",
        "    def recursive_ingest(self, url: str, base_url: str, depth: int) -> None:\n",
        "        \"\"\"Recursively crawl pages starting from the given URL.\"\"\"\n",
        "        url = normalize_url(url)\n",
        "        if url in self.visited:\n",
        "            return\n",
        "        self.visited.add(url)\n",
        "        \n",
        "        print(f\"\\n[DEPTH {depth}] Fetching: {url}\")\n",
        "        try:\n",
        "            page = fetch_page(url)\n",
        "        except Exception as e:\n",
        "            print(f\"    âœ— ERROR: {e}\")\n",
        "            return\n",
        "        \n",
        "        # Decide if page should be expanded\n",
        "        should_expand, reason = should_expand_page(page[\"text\"], url, self.llm)\n",
        "        \n",
        "        if should_expand:\n",
        "            print(f\"    âœ“ EXPAND: {reason}\")\n",
        "        else:\n",
        "            print(f\"    âœ— SKIP: {reason}\")\n",
        "        \n",
        "        # Save page with metadata\n",
        "        depth_dir = os.path.join(self.output_dir, f\"depth_{depth}\")\n",
        "        os.makedirs(depth_dir, exist_ok=True)\n",
        "        filename = (\n",
        "            urlparse(url).path.strip(\"/\")\n",
        "            .replace(\"/\", \"_\")\n",
        "            .replace(\".\", \"_\")\n",
        "            or \"root\"\n",
        "        )\n",
        "        out_path = os.path.join(depth_dir, f\"{filename}.json\")\n",
        "        page_json = {\n",
        "            **page,\n",
        "            \"crawl_depth\": depth,\n",
        "            \"parent_url\": base_url if depth > 0 else None,\n",
        "            \"should_expand\": should_expand,\n",
        "            \"expand_reason\": reason\n",
        "        }\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(page_json, f, ensure_ascii=False, indent=2)\n",
        "        \n",
        "        # Stop conditions\n",
        "        if depth >= self.max_depth:\n",
        "            print(f\"    â†’ Max depth reached\")\n",
        "            return\n",
        "        \n",
        "        if not should_expand:\n",
        "            return\n",
        "        \n",
        "        # Extract and filter links\n",
        "        links = extract_internal_links_with_anchor(page[\"html\"], url)\n",
        "        print(f\"    â†’ Extracted {len(links)} total links, filtering with AI...\")\n",
        "        relevant_links = filter_relevant_urls_with_ai(links, self.llm, batch_size=15)\n",
        "        print(f\"    â†’ Final: {len(relevant_links)} educational links to explore\")\n",
        "        \n",
        "        # Recursively crawl relevant links\n",
        "        for link in relevant_links:\n",
        "            next_url = normalize_url(link[\"url\"])\n",
        "            if urlparse(next_url).netloc != urlparse(base_url).netloc:\n",
        "                continue\n",
        "            if next_url not in self.visited:\n",
        "                time.sleep(self.rate_limit)\n",
        "                self.recursive_ingest(next_url, base_url, depth + 1)\n",
        "\n",
        "print(\"âœ“ Academic Crawler class defined\")"
      ],
      "metadata": {
        "id": "crawler_class"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 11: Run the Crawler"
      ],
      "metadata": {
        "id": "step11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "START_URL = \"https://cmb.ac.lk/\"  # Change to your target university\n",
        "MAX_DEPTH = 2  # Start with depth 2 for testing\n",
        "RATE_LIMIT = 2  # Seconds between requests\n",
        "OUTPUT_DIR = \"/content/crawled_pages\"\n",
        "\n",
        "print(f\"\"\"\n",
        "=============================\n",
        "University Academic Crawler\n",
        "=============================\n",
        "[CONFIG]\n",
        "  start_url  = {START_URL}\n",
        "  max_depth  = {MAX_DEPTH}\n",
        "  rate_limit = {RATE_LIMIT}s\n",
        "  output_dir = {OUTPUT_DIR}\n",
        "  model      = Llama 3.2 (Ollama)\n",
        "\n",
        "[GOAL]\n",
        "  Building database of:\n",
        "  - Educational courses & programs\n",
        "  - Admission criteria & requirements\n",
        "  - Certificates & diplomas\n",
        "  - Academic pathways & progressions\n",
        "=============================\n",
        "\"\"\")\n",
        "\n",
        "# Initialize and run crawler\n",
        "crawler = AcademicCrawler(\n",
        "    llm=llm,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    max_depth=MAX_DEPTH,\n",
        "    rate_limit=RATE_LIMIT\n",
        ")\n",
        "\n",
        "# Start crawling\n",
        "crawler.recursive_ingest(START_URL, START_URL, 0)\n",
        "\n",
        "print(f\"\"\"\n",
        "=============================\n",
        "[CRAWL COMPLETE]\n",
        "=============================\n",
        "Total pages crawled: {len(crawler.visited)}\n",
        "Output directory: {OUTPUT_DIR}\n",
        "=============================\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "run_crawler"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 12: Analyze Crawled Data"
      ],
      "metadata": {
        "id": "step12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from collections import Counter\n",
        "\n",
        "# Count files per depth\n",
        "depth_counts = {}\n",
        "for depth in range(MAX_DEPTH + 1):\n",
        "    depth_dir = os.path.join(OUTPUT_DIR, f\"depth_{depth}\")\n",
        "    if os.path.exists(depth_dir):\n",
        "        count = len([f for f in os.listdir(depth_dir) if f.endswith('.json')])\n",
        "        depth_counts[depth] = count\n",
        "\n",
        "print(\"\\nðŸ“Š Crawl Statistics:\")\n",
        "print(\"=\" * 40)\n",
        "for depth, count in sorted(depth_counts.items()):\n",
        "    print(f\"Depth {depth}: {count} pages\")\n",
        "print(f\"\\nTotal pages: {sum(depth_counts.values())}\")\n",
        "\n",
        "# Sample a crawled page\n",
        "json_files = glob.glob(os.path.join(OUTPUT_DIR, \"**/*.json\"), recursive=True)\n",
        "if json_files:\n",
        "    print(f\"\\nðŸ“„ Sample crawled page:\")\n",
        "    with open(json_files[0], 'r') as f:\n",
        "        sample = json.load(f)\n",
        "        print(f\"URL: {sample['url']}\")\n",
        "        print(f\"Depth: {sample['crawl_depth']}\")\n",
        "        print(f\"Should Expand: {sample['should_expand']}\")\n",
        "        print(f\"Reason: {sample.get('expand_reason', 'N/A')}\")\n",
        "        print(f\"Content length: {len(sample['text'])} chars\")"
      ],
      "metadata": {
        "id": "analyze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 13: Download Crawled Data (Optional)"
      ],
      "metadata": {
        "id": "step13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a zip file of all crawled data\n",
        "!zip -r /content/crawled_data.zip {OUTPUT_DIR}\n",
        "\n",
        "print(\"âœ“ Crawled data saved to: /content/crawled_data.zip\")\n",
        "print(\"You can download this file from the Files panel on the left.\")"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}