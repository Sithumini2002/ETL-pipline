{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43eb5afa",
   "metadata": {},
   "source": [
    "# ETL Pipeline: Extract Academic Program Data with Ollama LLM and MongoDB\n",
    "\n",
    "This notebook demonstrates an ETL (Extract, Transform, Load) pipeline that extracts academic program data from text files using the Ollama LLM and loads the structured data into MongoDB.\n",
    "\n",
    "**Outline:**\n",
    "1. Install and Import Required Packages\n",
    "2. Define Pydantic Models for Institution and Program\n",
    "3. Configure ETL Pipeline Settings\n",
    "4. Implement Ollama LLM Client for Data Extraction\n",
    "5. Set Up MongoDB Manager\n",
    "6. Define ETL Pipeline Logic\n",
    "7. Read and Process Text Files\n",
    "8. Run the ETL Pipeline and Display Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5898d4",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Packages\n",
    "\n",
    "Install the necessary Python packages (`pydantic`, `pymongo`, `requests`) and import all required modules for the ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd38910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package.split('[')[0])\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "packages = ['pydantic', 'pymongo', 'requests']\n",
    "for pkg in packages:\n",
    "    install_package(pkg)\n",
    "\n",
    "# Import all required modules\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Any\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import DuplicateKeyError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f0ba6",
   "metadata": {},
   "source": [
    "## 2. Define Pydantic Models for Institution and Program\n",
    "\n",
    "Create Pydantic models to validate and structure the data for institutions and academic programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Institution(BaseModel):\n",
    "    name: str = Field(..., description=\"Official institution name\")\n",
    "    institution_code: Optional[str] = Field(None, description=\"Unique identifier\")\n",
    "    description: Optional[str] = Field(None, description=\"Institution overview\")\n",
    "    type: List[str] = Field(default_factory=list, description=\"Institution types\")\n",
    "    country: str = Field(default=\"Sri Lanka\", description=\"Country\")\n",
    "    website: Optional[str] = Field(None, description=\"Website URL\")\n",
    "    recognition: Optional[Dict[str, Any]] = Field(None, description=\"Accreditation\")\n",
    "    contact_info: Optional[Dict[str, Any]] = Field(None, description=\"Contact details\")\n",
    "    confidence_score: float = Field(..., ge=0.0, le=1.0, description=\"Confidence (0-1)\")\n",
    "    model_config = ConfigDict(extra=\"forbid\")\n",
    "\n",
    "class Program(BaseModel):\n",
    "    name: str = Field(..., description=\"Program name\")\n",
    "    program_code: Optional[str] = Field(None, description=\"Unique identifier\")\n",
    "    description: Optional[str] = Field(None, description=\"Program overview\")\n",
    "    level: Optional[str] = Field(None, description=\"Academic level\")\n",
    "    duration: Optional[Dict[str, Any]] = Field(None, description=\"Duration details\")\n",
    "    delivery_mode: Optional[List[str]] = Field(None, description=\"Delivery modes\")\n",
    "    fees: Optional[Dict[str, Any]] = Field(None, description=\"Fee structure\")\n",
    "    eligibility: Optional[Dict[str, Any]] = Field(None, description=\"Requirements\")\n",
    "    curriculum_summary: Optional[str] = Field(None, description=\"Curriculum overview\")\n",
    "    specializations: Optional[List[str]] = Field(None, description=\"Specializations\")\n",
    "    extensions: Optional[Dict[str, Any]] = Field(None, description=\"Additional data\")\n",
    "    confidence_score: float = Field(..., ge=0.0, le=1.0, description=\"Confidence (0-1)\")\n",
    "    model_config = ConfigDict(extra=\"forbid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a72ba8",
   "metadata": {},
   "source": [
    "## 3. Configure ETL Pipeline Settings\n",
    "\n",
    "Set up configuration variables for the Ollama LLM API, MongoDB connection, and default values for missing fields in the extracted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bed329",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "    OLLAMA_MODEL = \"llama3.2\"  # Change to your model\n",
    "    MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "    DB_NAME = \"education_db\"\n",
    "    INSTITUTION_COLLECTION = \"institutions\"\n",
    "    DATA_FOLDER = \"data\"  # Folder containing text files\n",
    "    # Realistic defaults for missing fields\n",
    "    DEFAULT_VALUES = {\n",
    "        \"level\": \"Undergraduate\",\n",
    "        \"delivery_mode\": [\"On-campus\"],\n",
    "        \"duration\": {\"years\": 4, \"months\": 0},\n",
    "        \"fees\": {\"currency\": \"LKR\", \"amount\": \"To be determined\"},\n",
    "        \"eligibility\": {\"minimum_qualification\": \"A-Level or equivalent\"}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35f0a31",
   "metadata": {},
   "source": [
    "## 4. Implement Ollama LLM Client for Data Extraction\n",
    "\n",
    "Define a client class to interact with the Ollama LLM API and extract academic program data from text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e7e549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaClient:\n",
    "    def __init__(self, base_url: str = Config.OLLAMA_BASE_URL, model: str = Config.OLLAMA_MODEL):\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        \n",
    "    def extract_program_data(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract program data from text using Ollama\"\"\"\n",
    "        prompt = f\"\"\"You are a data extraction assistant. Analyze the following text and extract academic program information.\\n\\nText:\\n{text}\\n\\nExtract the following information in JSON format:\\n- name: Program name (required)\\n- program_code: Program code if mentioned\\n- description: Brief description\\n- level: Academic level (Undergraduate/Postgraduate/Diploma/Certificate)\\n- duration: Duration in years/months as {{\\\"years\\\": X, \\\"months\\\": Y}}\\n- delivery_mode: List of delivery modes (On-campus/Online/Hybrid)\\n- fees: Fee information as {{\\\"currency\\\": \\\"XXX\\\", \\\"amount\\\": \\\"value\\\"}}\\n- eligibility: Eligibility requirements as {{\\\"minimum_qualification\\\": \\\"...\\\"}}\\n- curriculum_summary: Brief curriculum overview\\n- specializations: List of specializations if any\\n- confidence_score: Your confidence in this extraction (0.0 to 1.0)\\n\\nIf the text does NOT contain program information, respond with: {\\\"is_program\\\": false}\\n\\nRespond ONLY with valid JSON, no additional text.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"stream\": False,\n",
    "                    \"format\": \"json\"\n",
    "                },\n",
    "                timeout=60\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            extracted = json.loads(result['response'])\n",
    "            return extracted\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Ollama extraction error: {e}\")\n",
    "            return {\"is_program\": False, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be382662",
   "metadata": {},
   "source": [
    "## 5. Set Up MongoDB Manager\n",
    "\n",
    "Create a class to manage MongoDB connections and operations, including inserting institutions and adding programs to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MongoDBManager:\n",
    "    def __init__(self, uri: str = Config.MONGO_URI, db_name: str = Config.DB_NAME):\n",
    "        self.client = MongoClient(uri)\n",
    "        self.db = self.client[db_name]\n",
    "        self.institutions = self.db[Config.INSTITUTION_COLLECTION]\n",
    "        \n",
    "    def create_institution(self, name: str, **kwargs) -> str:\n",
    "        \"\"\"Create institution and return its ID\"\"\"\n",
    "        institution = Institution(\n",
    "            name=name,\n",
    "            confidence_score=kwargs.get('confidence_score', 1.0),\n",
    "            **{k: v for k, v in kwargs.items() if k != 'confidence_score'}\n",
    "        )\n",
    "        \n",
    "        institution_dict = institution.model_dump()\n",
    "        result = self.institutions.insert_one(institution_dict)\n",
    "        print(f\"‚úÖ Created institution: {name} (ID: {result.inserted_id})\")\n",
    "        return str(result.inserted_id)\n",
    "    \n",
    "    def add_program_to_institution(self, institution_id: str, program: Program):\n",
    "        \"\"\"Add program to institution using $push\"\"\"\n",
    "        from bson.objectid import ObjectId\n",
    "        \n",
    "        program_dict = program.model_dump()\n",
    "        \n",
    "        result = self.institutions.update_one(\n",
    "            {\"_id\": ObjectId(institution_id)},\n",
    "            {\"$push\": {\"programs\": program_dict}}\n",
    "        )\n",
    "        \n",
    "        if result.modified_count > 0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close MongoDB connection\"\"\"\n",
    "        self.client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ad53b",
   "metadata": {},
   "source": [
    "## 6. Define ETL Pipeline Logic\n",
    "\n",
    "Implement the `ETLPipeline` class to orchestrate reading files, extracting data, filling missing fields, and loading the results into MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aba0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLPipeline:\n",
    "    def __init__(self):\n",
    "        self.ollama = OllamaClient()\n",
    "        self.db = MongoDBManager()\n",
    "        self.stats = {\n",
    "            'total_files': 0,\n",
    "            'processed': 0,\n",
    "            'skipped': 0,\n",
    "            'errors': 0,\n",
    "            'programs_added': 0\n",
    "        }\n",
    "    \n",
    "    def read_text_file(self, filepath: Path) -> str:\n",
    "        \"\"\"Read text file content\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                return f.read().strip()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading {filepath}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def fill_missing_fields(self, program_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Fill missing fields with realistic defaults\"\"\"\n",
    "        for field, default in Config.DEFAULT_VALUES.items():\n",
    "            if field not in program_data or program_data[field] is None:\n",
    "                program_data[field] = default\n",
    "                print(f\"  üìù Filled missing field '{field}' with default value\")\n",
    "        return program_data\n",
    "    \n",
    "    def process_file(self, filepath: Path, institution_id: str):\n",
    "        \"\"\"Process single text file\"\"\"\n",
    "        print(f\"\\nüìÑ Processing: {filepath.name}\")\n",
    "        \n",
    "        text = self.read_text_file(filepath)\n",
    "        if not text:\n",
    "            print(f\"  ‚ö†Ô∏è  Empty file, skipping...\")\n",
    "            self.stats['skipped'] += 1\n",
    "            return\n",
    "        \n",
    "        # Extract data using Ollama\n",
    "        print(f\"  ü§ñ Extracting data with Ollama...\")\n",
    "        extracted = self.ollama.extract_program_data(text)\n",
    "        \n",
    "        # Check if it's program-related\n",
    "        if not extracted.get('is_program', True):\n",
    "            print(f\"  ‚è≠Ô∏è  Not program-related, skipping...\")\n",
    "            self.stats['skipped'] += 1\n",
    "            return\n",
    "        \n",
    "        # Fill missing fields\n",
    "        extracted = self.fill_missing_fields(extracted)\n",
    "        \n",
    "        # Ensure required fields\n",
    "        if 'name' not in extracted or not extracted['name']:\n",
    "            print(f\"  ‚ùå No program name found, skipping...\")\n",
    "            self.stats['skipped'] += 1\n",
    "            return\n",
    "        \n",
    "        # Ensure confidence score\n",
    "        if 'confidence_score' not in extracted:\n",
    "            extracted['confidence_score'] = 0.7\n",
    "        \n",
    "        try:\n",
    "            # Create Program object\n",
    "            program = Program(**extracted)\n",
    "            \n",
    "            # Add to MongoDB\n",
    "            success = self.db.add_program_to_institution(institution_id, program)\n",
    "            \n",
    "            if success:\n",
    "                print(f\"  ‚úÖ Added program: {program.name} (confidence: {program.confidence_score:.2f})\")\n",
    "                self.stats['programs_added'] += 1\n",
    "                self.stats['processed'] += 1\n",
    "            else:\n",
    "                print(f\"  ‚ùå Failed to add program to database\")\n",
    "                self.stats['errors'] += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error creating program: {e}\")\n",
    "            self.stats['errors'] += 1\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the ETL pipeline\"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"ETL PIPELINE: Text Files ‚Üí Ollama ‚Üí MongoDB\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Step 1: Get institution name from user\n",
    "        institution_name = input(\"\\nüèõÔ∏è  Enter institution name: \").strip()\n",
    "        if not institution_name:\n",
    "            print(\"‚ùå Institution name is required!\")\n",
    "            return\n",
    "        \n",
    "        # Step 2: Create institution\n",
    "        print(f\"\\nüìã Creating institution...\")\n",
    "        institution_id = self.db.create_institution(institution_name)\n",
    "        \n",
    "        # Step 3: Get all text files\n",
    "        data_folder = Path(Config.DATA_FOLDER)\n",
    "        if not data_folder.exists():\n",
    "            print(f\"‚ùå Data folder '{Config.DATA_FOLDER}' not found!\")\n",
    "            return\n",
    "        \n",
    "        text_files = list(data_folder.glob(\"*.txt\"))\n",
    "        self.stats['total_files'] = len(text_files)\n",
    "        \n",
    "        if not text_files:\n",
    "            print(f\"‚ùå No .txt files found in '{Config.DATA_FOLDER}'\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nüìö Found {len(text_files)} text files\")\n",
    "        \n",
    "        # Step 4: Process each file\n",
    "        for filepath in text_files:\n",
    "            self.process_file(filepath, institution_id)\n",
    "        \n",
    "        # Step 5: Print summary\n",
    "        self.print_summary()\n",
    "        \n",
    "        # Step 6: Cleanup\n",
    "        self.db.close()\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print pipeline execution summary\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"PIPELINE SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Total files:       {self.stats['total_files']}\")\n",
    "        print(f\"Processed:         {self.stats['processed']}\")\n",
    "        print(f\"Skipped:           {self.stats['skipped']}\")\n",
    "        print(f\"Errors:            {self.stats['errors']}\")\n",
    "        print(f\"Programs added:    {self.stats['programs_added']}\")\n",
    "        print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da0737",
   "metadata": {},
   "source": [
    "## 7. Read and Process Text Files\n",
    "\n",
    "Read text files from the data folder, process each file, and extract program information using the ETL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Read and process all text files in the data folder\n",
    "# (This is handled inside ETLPipeline.run(), but you can call process_file directly if needed)\n",
    "#\n",
    "# pipeline = ETLPipeline()\n",
    "# data_folder = Path(Config.DATA_FOLDER)\n",
    "# for filepath in data_folder.glob('*.txt'):\n",
    "#     pipeline.process_file(filepath, institution_id)\n",
    "#\n",
    "# For full pipeline execution, see the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e71c0ea",
   "metadata": {},
   "source": [
    "## 8. Run the ETL Pipeline and Display Summary\n",
    "\n",
    "Execute the ETL pipeline, prompt for the institution name, process all files, and print a summary of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f530d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ETL pipeline interactively\n",
    "pipeline = ETLPipeline()\n",
    "pipeline.run()\n",
    "# The pipeline will prompt for the institution name, process all text files in the data folder, and print a summary."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
